100%|████████████████████████████████████████████████████████████| 15000/15000 [00:00<00:00, 35905.32it/s]
NLST Dataset. 28161 exams (417.0 with cancer in one year, 1444 cancer ever) from 9646 patients
NLST Dataset. 6839 exams (99.0 with cancer in one year, 337 cancer ever) from 2336 patients
NLST Dataset. 6479 exams (86.0 with cancer in one year, 320 cancer ever) from 2204 patients
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name       | Type               | Params | Mode
----------------------------------------------------------
0 | loss       | CrossEntropyLoss   | 0      | train
1 | accuracy   | MulticlassAccuracy | 0      | train
2 | auc        | MulticlassAUROC    | 0      | train
3 | classifier | ResNet             | 11.2 M | train
----------------------------------------------------------
11.2 M    Trainable params
0         Non-trainable params
11.2 M    Total params
44.725    Total estimated model params size (MB)
71        Modules in train mode
0         Modules in eval mode
Epoch 0:  29%|██▎     | 4089/14081 [27:41<1:07:39,  2.46it/s, v_num=3hgp, train_acc=1.000, train_loss=0.0119]
/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score
  warnings.warn(*args, **kwargs)  # noqa: B028
/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
  warnings.warn(*args, **kwargs)  # noqa: B028
Traceback (most recent call last):
  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/multiprocessing/queues.py", line 113, in get
    if not self._poll(timeout):
  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/multiprocessing/connection.py", line 257, in poll
    return self._poll(timeout)
  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/multiprocessing/connection.py", line 424, in _poll
    r = wait([self], timeout)
  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/selectors.py", line 416, in select
    fd_event_list = self._selector.poll(timeout)
  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1912738) is killed by signal: Killed.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py", line 337, in <module>
    main(args)
  File "/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py", line 317, in main
    trainer.fit(model, datamodule)
  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
    self.fit_loop.run()
  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
    self.advance(data_fetcher)
  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 212, in advance
    batch, _, __ = next(data_fetcher)
  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
    out[i] = next(self.iterators[i])
  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
    data = self._next_data()
  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
    idx, data = self._get_data()
  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1412, in _get_data
    success, data = self._try_get_data()
  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
    raise RuntimeError(
RuntimeError: DataLoader worker (pid(s) 1912738) exited unexpectedly
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank0]:     data = self._data_queue.get(timeout=timeout)
[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/multiprocessing/queues.py", line 113, in get
[rank0]:     if not self._poll(timeout):
[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/multiprocessing/connection.py", line 257, in poll
[rank0]:     return self._poll(timeout)
[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/multiprocessing/connection.py", line 424, in _poll
[rank0]:     r = wait([self], timeout)
[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/multiprocessing/connection.py", line 931, in wait
[rank0]:     ready = selector.select(timeout)
[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/selectors.py", line 416, in select
[rank0]:     fd_event_list = self._selector.poll(timeout)
[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank0]:     _error_if_any_worker_fails()
[rank0]: RuntimeError: DataLoader worker (pid 1912738) is killed by signal: Killed.

[rank0]: The above exception was the direct cause of the following exception:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py", line 337, in <module>
[rank0]:     main(args)
[rank0]:   File "/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py", line 317, in main
[rank0]:     trainer.fit(model, datamodule)
[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
[rank0]:     call._call_and_handle_interrupt(
[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank0]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank0]:     return function(*args, **kwargs)
[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
[rank0]:     self._run(model, ckpt_path=ckpt_path)
[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
[rank0]:     results = self._run_stage()
[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
[rank0]:     self.fit_loop.run()
[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
[rank0]:     self.advance()
[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
[rank0]:     self.epoch_loop.run(self._data_fetcher)
[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
[rank0]:     self.advance(data_fetcher)
[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 212, in advance
[rank0]:     batch, _, __ = next(data_fetcher)
[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/fetchers.py", line 133, in __next__
[rank0]:     batch = super().__next__()
[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/fetchers.py", line 60, in __next__
[rank0]:     batch = next(self.iterator)
[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank0]:     out = next(self._iterator)
[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank0]:     out[i] = next(self.iterators[i])
[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank0]:     idx, data = self._get_data()
[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1412, in _get_data
[rank0]:     success, data = self._try_get_data()
[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank0]:     raise RuntimeError(
[rank0]: RuntimeError: DataLoader worker (pid(s) 1912738) exited unexpectedly
