diff --git a/.gitignore b/.gitignore
index b42097e..4f509e6 100644
--- a/.gitignore
+++ b/.gitignore
@@ -1 +1 @@
-**/__pycache__/
\ No newline at end of file
+**/__pycache__//scratch/users/rbhalerao/CPH200A_project2/wandb
diff --git a/run_resnet18_adapt.sh b/run_resnet18_adapt.sh
index 803fc7c..d9cd9d8 100644
--- a/run_resnet18_adapt.sh
+++ b/run_resnet18_adapt.sh
@@ -1,11 +1,11 @@
-python /scratch/users/rbhalerao/CPH200A_project2/scripts/main.py \
+CUDA_VISIBLE_DEVICES=1 python /scratch/users/rbhalerao/CPH200A_project2/scripts/main.py \
     --model_name resnet_adapt \
     --dataset_name nlst \
     --train True \
     --pretraining True \
     --use_data_augmentation False \
     --batch_size 2 \
-    --num_workers 4 \
+    --num_workers 0 \
     --project_name "cornerstone_project_2" \
     --wandb_entity "cph29" \
     --depth_handling "max_pool" 
diff --git a/scripts/main.py b/scripts/main.py
index 5d4588f..1ae81ad 100644
--- a/scripts/main.py
+++ b/scripts/main.py
@@ -14,6 +14,7 @@ import wandb
 from torch.backends import cudnn
 from lightning import seed_everything
 import json
+import torch
 
 dirname = os.path.dirname(__file__)
 global_seed = json.load(open(os.path.join(dirname, '..', 'global_seed.json')))['global_seed']
@@ -309,6 +310,7 @@ def main(args: argparse.Namespace):
     logger = get_logger(args)
     callbacks = get_callbacks(args)
     trainer = get_trainer(args, callbacks=callbacks, logger=logger)
+    torch.cuda.set_per_process_memory_fraction(0.8, device=0)
 
     if args.train:
         print("Training model")
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
deleted file mode 120000
index 128d197..0000000
--- a/wandb/debug-internal.log
+++ /dev/null
@@ -1 +0,0 @@
-run-20241114_195653-snr5hkox/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
deleted file mode 120000
index 63781c6..0000000
--- a/wandb/debug.log
+++ /dev/null
@@ -1 +0,0 @@
-run-20241114_195653-snr5hkox/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
deleted file mode 120000
index 08c0084..0000000
--- a/wandb/latest-run
+++ /dev/null
@@ -1 +0,0 @@
-run-20241114_195653-snr5hkox
\ No newline at end of file
diff --git a/wandb/run-20241114_074457-qnoibns1/files/code/scripts/main.py b/wandb/run-20241114_074457-qnoibns1/files/code/scripts/main.py
deleted file mode 100644
index 8ac4d07..0000000
--- a/wandb/run-20241114_074457-qnoibns1/files/code/scripts/main.py
+++ /dev/null
@@ -1,315 +0,0 @@
-import inspect
-import math
-import argparse
-import sys
-import os
-
-sys.path.append(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))
-from src.lightning import MLP, CNN, LinearModel, ResNet18, RiskModel, ResNet3D, Swin3DModel
-from src.dataset import PathMnist, NLST
-from lightning.pytorch.cli import LightningArgumentParser
-import lightning.pytorch as pl
-from torch.cuda import device_count
-import wandb
-from torch.backends import cudnn
-from lightning import seed_everything
-import json
-
-dirname = os.path.dirname(__file__)
-global_seed = json.load(open(os.path.join(dirname, '..', 'global_seed.json')))['global_seed']
-seed_everything(global_seed)
-cudnn.benchmark = False
-
-NAME_TO_MODEL_CLASS = {
-    "mlp": MLP,
-    "cnn": CNN,
-    # "cnn3d": CNN3D,
-    "linear": LinearModel,
-    "resnet": ResNet18,
-    "resnet3d": ResNet3D,
-    "swin3d": Swin3DModel,
-    "risk_model": RiskModel
-}
-
-
-NAME_TO_DATASET_CLASS = {
-    "pathmnist": PathMnist,
-    "nlst": NLST
-}
-
-MODEL_TO_DATASET = {
-    "mlp": "pathmnist",
-    "cnn": "pathmnist",
-    "cnn3d": "nlst",
-    "linear": "pathmnist",
-    "resnet": "pathmnist",
-    "resnet3d": "nlst",
-    "swin3d": "nlst",
-    "risk_model": "nlst"
-}
-
-dirname = os.path.dirname(__file__)
-
-def add_main_args(parser: LightningArgumentParser) -> LightningArgumentParser:
-
-    parser.add_argument(
-        "--model_name",
-        default="mlp",
-        choices=["mlp", "linear", "cnn", "resnet", "risk_model", "swin3d", "resnet3d"],  
-        help="Name of model to use",
-    )
-
-    parser.add_argument(
-        "--dataset_name",
-        default="pathmnist",
-        choices=["pathmnist", "nlst"],
-        help="Name of dataset to use"
-    )
-
-    parser.add_argument(
-        "--project_name",
-        default="cornerstone_project_2",
-        help="Name of project for wandb"
-    )
-
-    parser.add_argument(
-        "--monitor_key",
-        default="val_loss",
-        help="Name of metric to use for checkpointing. (e.g. val_loss, val_acc)"
-    )
-
-    parser.add_argument(
-        "--checkpoint_path",
-        default=None,
-        help="Path to checkpoint to load from. If None, init from scratch."
-    )
-
-    parser.add_argument(
-        "--train",
-        default=False,
-        type=bool,
-        help="Whether to train the model."
-    )
-
-    parser.add_argument(
-        "--num_layers",
-        default=1,
-        type=int,
-        help="Depth of the model (number of layers)",
-    )
-
-    parser.add_argument(
-        "--use_bn",
-        default=False,
-        type=bool,
-        help="Whether to batch normalize in each layer",
-    )
-
-    parser.add_argument(
-        "--hidden_dim",
-        default=512,
-        type=int,
-        help="The dimension of the hidden layer(s)"
-    )
-
-    parser.add_argument(
-        "--use_data_augmentation",
-        default=False,
-        type=bool,
-        help="Whether to augment the data"
-    )
-
-    parser.add_argument(
-        "--pretraining",
-        default=False,
-        type=bool,
-        help="Whether to use pretrained model weights (only used for resnet)"
-    )
-
-    parser.add_argument(
-        "--wandb_entity",
-        default='CPH29',
-        type=str,
-        help="The wandb account to log metrics and models to"
-    )
-
-    parser.add_argument(
-        "--num_workers",
-        type=int,
-        default=1,
-        help="Number of processes to running in parallel"
-    )
-
-    parser.add_argument(
-        "--class_balance",
-        default=False,
-        type=bool,
-        help="Whether to perform class-balanced sampling (only used for nlst dataset)"
-    )
-
-    parser.add_argument(
-        "--batch_size",
-        default=4,
-        type=int,
-        help="Number of samples per batch"
-    )
-
-    parser.add_argument(
-        "--group_keys",
-        default=['race', 'educat', 'gender', 'age', 'ethnic'],
-        nargs='*',
-        help="The groups to perform subgroup analysis on (only used for nlst dataset)"
-    )
-
-    return parser
-
-def parse_args() -> argparse.Namespace:
-    parser = LightningArgumentParser()
-    parser.add_lightning_class_args(pl.Trainer, nested_key="trainer")
-    for model_name, model_class in NAME_TO_MODEL_CLASS.items():
-        parser.add_lightning_class_args(model_class, nested_key=model_name)
-    for dataset_name, data_class in NAME_TO_DATASET_CLASS.items():
-        parser.add_lightning_class_args(data_class, nested_key=dataset_name)
-    parser = add_main_args(parser)
-    args = parser.parse_args()
-    return args
-
-def get_caller():
-    caller = os.path.split(inspect.getsourcefile(sys._getframe(1)))[-1]
-    return caller
-
-def get_datamodule_num_workers(num_process_workers=None):
-    # set per https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813/5
-    num_process_workers = num_process_workers if num_process_workers else 1
-    datamodule_num_workers = device_count() * 8
-    n_cpus = os.cpu_count()
-    if datamodule_num_workers * num_process_workers >= n_cpus:
-        datamodule_num_workers = math.floor(n_cpus/num_process_workers * .9) 
-    return datamodule_num_workers
-
-def get_datamodule(args):
-    # get workers for datamodule
-    datamodule_num_workers = get_datamodule_num_workers(args.num_workers)
-    
-    # get datamodule args
-    datamodule_vars = vars(vars(args)[args.dataset_name])
-    update_vars = {k:v for k,v in vars(args).items() if k in datamodule_vars}
-    datamodule_vars.update(update_vars)
-    datamodule_vars.update({'num_workers': datamodule_num_workers})
-
-    # init data module
-    datamodule = NAME_TO_DATASET_CLASS[args.dataset_name](**datamodule_vars)
-
-    return datamodule
-
-def get_model(args):
-    print(f"Initializing {args.model_name} model")
-    if args.checkpoint_path is None:
-        model_vars = vars(vars(args)[args.model_name])
-        update_vars = {k:v for k,v in vars(args).items() if k in model_vars}
-        model_vars.update(update_vars)
-        print('with params ', model_vars)
-        model = NAME_TO_MODEL_CLASS[args.model_name](**model_vars)
-    else:
-        model = NAME_TO_MODEL_CLASS[args.model_name].load_from_checkpoint(args.checkpoint_path)
-
-    return model
-
-def get_trainer(args, strategy='ddp', logger=None, callbacks=[]):
-    args.trainer.accelerator = 'auto'
-    args.trainer.strategy = strategy
-    args.trainer.logger = logger
-    args.trainer.precision = "bf16-mixed" ## This mixed precision training is highly recommended
-    args.trainer.min_epochs = 100
-
-    # set checkpoint save directory
-    dirpath = os.path.join(dirname, '../models', args.model_name)
-    if not os.path.isdir(dirpath):
-        os.makedirs(dirpath)
-
-    args.trainer.callbacks = callbacks
-
-    # init trainer
-    trainer_args = vars(args.trainer)
-    trainer = pl.Trainer(**trainer_args)
-
-    return trainer
-
-def get_logger(args):
-    logger = pl.loggers.WandbLogger(project=args.project_name,
-                                    entity=args.wandb_entity,
-                                    group=args.model_name,
-                                    dir=os.path.join(dirname, '..'))
-
-    return logger
-
-def get_callbacks(args):
-    # set checkpoint save directory
-    dirpath = os.path.join(dirname, '../models', args.model_name)
-    if not os.path.isdir(dirpath):
-        os.makedirs(dirpath)
-    
-    callbacks = [
-        pl.callbacks.ModelCheckpoint(
-            monitor=args.monitor_key,
-            mode='min' if "loss" in args.monitor_key else "max",
-            dirpath=dirpath,
-            filename=args.model_name + '-{epoch:002d}-{val_loss:.2f}',
-            save_last=True,
-            every_n_epochs=1
-        ),
-        pl.callbacks.EarlyStopping(
-            monitor=args.monitor_key,
-            mode='min' if "loss" in args.monitor_key else "max",
-            patience=10,
-            check_on_train_epoch_end=True
-        )]
-    
-    return callbacks
-
-
-def main(args: argparse.Namespace):
-    print("Loading data ..")
-    print(f"Training: {args.train}")
-    print(f"Model Name: {args.model_name}")
-    print(f"Dataset Name: {args.dataset_name}")
-    print(f"Pretraining: {args.pretraining}")
-    print(f"Num Workers: {args.num_workers}")
-
-    print("Preparing lighning data module (encapsulates dataset init and data loaders)")
-    """
-        Most the data loading logic is pre-implemented in the LightningDataModule class for you.
-        However, you may want to alter this code for special localization logic or to suit your risk
-        model implementations
-    """
-
-    datamodule = get_datamodule(args)
-    model = get_model(args)
-    logger = get_logger(args)
-    callbacks = get_callbacks(args)
-    trainer = get_trainer(args, callbacks=callbacks, logger=logger)
-
-    if args.train:
-        print("Training model")
-        trainer.fit(model, datamodule)
-
-    print("Best model checkpoint path: ", trainer.checkpoint_callback.best_model_path)
-
-    print("Evaluating model on validation set")
-    trainer.validate(model, datamodule)
-
-    print("Evaluating model on test set")
-    trainer.test(model, datamodule)
-
-    if logger:
-        logger.finalize('success')
-    wandb.finish()
-
-    print("Done")
-
-
-if __name__ == '__main__':
-    __spec__ = None
-    args = parse_args()
-    main(args)
-
diff --git a/wandb/run-20241114_074457-qnoibns1/files/config.yaml b/wandb/run-20241114_074457-qnoibns1/files/config.yaml
deleted file mode 100644
index 536dafb..0000000
--- a/wandb/run-20241114_074457-qnoibns1/files/config.yaml
+++ /dev/null
@@ -1,80 +0,0 @@
-_wandb:
-    value:
-        cli_version: 0.18.7
-        code_path: code/scripts/main.py
-        m:
-            - "1": trainer/global_step
-              "6":
-                - 3
-              "7": []
-        python_version: 3.10.15
-        t:
-            "1":
-                - 1
-                - 5
-                - 41
-                - 50
-                - 53
-                - 55
-                - 106
-            "2":
-                - 1
-                - 5
-                - 41
-                - 50
-                - 53
-                - 55
-                - 106
-            "3":
-                - 7
-                - 23
-                - 55
-                - 66
-            "4": 3.10.15
-            "5": 0.18.7
-            "8":
-                - 5
-            "12": 0.18.7
-            "13": linux-x86_64
-batch_size:
-    value: 4
-class_balance:
-    value: false
-feature_config:
-    value:
-        - age
-group_keys:
-    value:
-        - race
-        - educat
-        - gender
-        - age
-        - ethnic
-img_size:
-    value:
-        - 256
-        - 256
-init_lr:
-    value: 0.001
-lungrads_path:
-    value: /scratch/project2/nlst-metadata/nlst_acc2lungrads.p
-max_followup:
-    value: 6
-nlst_dir:
-    value: /scratch/project2/compressed
-nlst_metadata_path:
-    value: /scratch/project2/nlst-metadata/full_nlst_google.json
-num_channels:
-    value: 3
-num_classes:
-    value: 9
-num_images:
-    value: 200
-num_workers:
-    value: 14
-pretraining:
-    value: true
-use_data_augmentation:
-    value: false
-valid_exam_path:
-    value: /scratch/project2/nlst-metadata/valid_exams.p
diff --git a/wandb/run-20241114_074457-qnoibns1/files/output.log b/wandb/run-20241114_074457-qnoibns1/files/output.log
deleted file mode 100644
index fe0a74d..0000000
--- a/wandb/run-20241114_074457-qnoibns1/files/output.log
+++ /dev/null
@@ -1,120 +0,0 @@
-100%|███████████████████████████████████████████████████████████████████████| 15000/15000 [00:00<00:00, 35695.82it/s]
-NLST Dataset. 28161 exams (417.0 with cancer in one year, 1444 cancer ever) from 9646 patients
-NLST Dataset. 6839 exams (99.0 with cancer in one year, 337 cancer ever) from 2336 patients
-NLST Dataset. 6479 exams (86.0 with cancer in one year, 320 cancer ever) from 2204 patients
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
-
-  | Name       | Type               | Params | Mode
-----------------------------------------------------------
-0 | loss       | CrossEntropyLoss   | 0      | train
-1 | accuracy   | MulticlassAccuracy | 0      | train
-2 | auc        | MulticlassAUROC    | 0      | train
-3 | classifier | ResNet             | 11.2 M | train
-----------------------------------------------------------
-11.2 M    Trainable params
-0         Non-trainable params
-11.2 M    Total params
-44.725    Total estimated model params size (MB)
-71        Modules in train mode
-0         Modules in eval mode
-Sanity Checking DataLoader 0:   0%|                                                            | 0/2 [00:00<?, ?it/s]
-Traceback (most recent call last):
-  File "/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py", line 314, in <module>
-    main(args)
-  File "/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py", line 294, in main
-    trainer.fit(model, datamodule)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
-    call._call_and_handle_interrupt(
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
-    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
-    return function(*args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
-    self._run(model, ckpt_path=ckpt_path)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
-    results = self._run_stage()
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1023, in _run_stage
-    self._run_sanity_check()
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1052, in _run_sanity_check
-    val_loop.run()
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py", line 178, in _decorator
-    return loop_run(self, *args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 135, in run
-    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 396, in _evaluation_step
-    output = call._call_strategy_hook(trainer, hook_name, *step_args)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 319, in _call_strategy_hook
-    output = fn(*args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 410, in validation_step
-    return self._forward_redirection(self.model, self.lightning_module, "validation_step", *args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 640, in __call__
-    wrapper_output = wrapper_module(*args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
-    return self._call_impl(*args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
-    return forward_call(*args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
-    else self._run_ddp_forward(*inputs, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
-    return self.module(*inputs, **kwargs)  # type: ignore[index]
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
-    return self._call_impl(*args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
-    return forward_call(*args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 633, in wrapped_forward
-    out = method(*_args, **_kwargs)
-  File "/scratch/users/rbhalerao/CPH200A_project2/src/lightning.py", line 71, in validation_step
-    y_hat = self.forward(x)
-  File "/scratch/users/rbhalerao/CPH200A_project2/src/lightning.py", line 457, in forward
-    batch_size, channels, width, height = x.size()
-ValueError: too many values to unpack (expected 4)
-[rank0]: Traceback (most recent call last):
-[rank0]:   File "/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py", line 314, in <module>
-[rank0]:     main(args)
-[rank0]:   File "/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py", line 294, in main
-[rank0]:     trainer.fit(model, datamodule)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
-[rank0]:     call._call_and_handle_interrupt(
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
-[rank0]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
-[rank0]:     return function(*args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
-[rank0]:     self._run(model, ckpt_path=ckpt_path)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
-[rank0]:     results = self._run_stage()
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1023, in _run_stage
-[rank0]:     self._run_sanity_check()
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1052, in _run_sanity_check
-[rank0]:     val_loop.run()
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py", line 178, in _decorator
-[rank0]:     return loop_run(self, *args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 135, in run
-[rank0]:     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 396, in _evaluation_step
-[rank0]:     output = call._call_strategy_hook(trainer, hook_name, *step_args)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 319, in _call_strategy_hook
-[rank0]:     output = fn(*args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 410, in validation_step
-[rank0]:     return self._forward_redirection(self.model, self.lightning_module, "validation_step", *args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 640, in __call__
-[rank0]:     wrapper_output = wrapper_module(*args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
-[rank0]:     return self._call_impl(*args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
-[rank0]:     return forward_call(*args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
-[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
-[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
-[rank0]:     return self._call_impl(*args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
-[rank0]:     return forward_call(*args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 633, in wrapped_forward
-[rank0]:     out = method(*_args, **_kwargs)
-[rank0]:   File "/scratch/users/rbhalerao/CPH200A_project2/src/lightning.py", line 71, in validation_step
-[rank0]:     y_hat = self.forward(x)
-[rank0]:   File "/scratch/users/rbhalerao/CPH200A_project2/src/lightning.py", line 457, in forward
-[rank0]:     batch_size, channels, width, height = x.size()
-[rank0]: ValueError: too many values to unpack (expected 4)
diff --git a/wandb/run-20241114_074457-qnoibns1/files/wandb-metadata.json b/wandb/run-20241114_074457-qnoibns1/files/wandb-metadata.json
deleted file mode 100644
index 9beaf5d..0000000
--- a/wandb/run-20241114_074457-qnoibns1/files/wandb-metadata.json
+++ /dev/null
@@ -1,81 +0,0 @@
-{
-  "os": "Linux-6.8.0-1014-nvidia-x86_64-with-glibc2.35",
-  "python": "3.10.15",
-  "startedAt": "2024-11-14T07:44:57.815495Z",
-  "args": [
-    "--model_name",
-    "resnet",
-    "--dataset_name",
-    "nlst",
-    "--train",
-    "True",
-    "--pretraining",
-    "True",
-    "--use_data_augmentation",
-    "False",
-    "--batch_size",
-    "4",
-    "--num_workers",
-    "4",
-    "--project_name",
-    "cornerstone_project_2",
-    "--wandb_entity",
-    "cph29"
-  ],
-  "program": "/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py",
-  "codePath": "scripts/main.py",
-  "git": {
-    "remote": "https://github.com/radhi-bhalerao/CPH200A_project2.git",
-    "commit": "3eca8d6930aef344f1fe58ce268871cfa453ac07"
-  },
-  "email": "rbh@berkeley.edu",
-  "root": ".",
-  "host": "cph-dept-02",
-  "username": "rbhalerao",
-  "executable": "/home/rbhalerao/miniconda3/envs/shree/bin/python",
-  "codePathLocal": "scripts/main.py",
-  "cpu_count": 32,
-  "cpu_count_logical": 64,
-  "gpu": "NVIDIA A40",
-  "gpu_count": 4,
-  "disk": {
-    "/": {
-      "total": "105089261568",
-      "used": "30846955520"
-    }
-  },
-  "memory": {
-    "total": "540682158080"
-  },
-  "cpu": {
-    "count": 32,
-    "countLogical": 64
-  },
-  "gpu_nvidia": [
-    {
-      "name": "NVIDIA A40",
-      "memoryTotal": "48305799168",
-      "cudaCores": 10752,
-      "architecture": "Ampere"
-    },
-    {
-      "name": "NVIDIA A40",
-      "memoryTotal": "48305799168",
-      "cudaCores": 10752,
-      "architecture": "Ampere"
-    },
-    {
-      "name": "NVIDIA A40",
-      "memoryTotal": "48305799168",
-      "cudaCores": 10752,
-      "architecture": "Ampere"
-    },
-    {
-      "name": "NVIDIA A40",
-      "memoryTotal": "48305799168",
-      "cudaCores": 10752,
-      "architecture": "Ampere"
-    }
-  ],
-  "cudaVersion": "12.6"
-}
\ No newline at end of file
diff --git a/wandb/run-20241114_074457-qnoibns1/files/wandb-summary.json b/wandb/run-20241114_074457-qnoibns1/files/wandb-summary.json
deleted file mode 100644
index 9cfad56..0000000
--- a/wandb/run-20241114_074457-qnoibns1/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"_wandb":{"runtime":63}}
\ No newline at end of file
diff --git a/wandb/run-20241114_074457-qnoibns1/logs/debug-core.log b/wandb/run-20241114_074457-qnoibns1/logs/debug-core.log
deleted file mode 120000
index d6e83b3..0000000
--- a/wandb/run-20241114_074457-qnoibns1/logs/debug-core.log
+++ /dev/null
@@ -1 +0,0 @@
-/home/rbhalerao/.cache/wandb/logs/core-debug-20241114_074338.log
\ No newline at end of file
diff --git a/wandb/run-20241114_074457-qnoibns1/logs/debug-internal.log b/wandb/run-20241114_074457-qnoibns1/logs/debug-internal.log
deleted file mode 100644
index 123ff44..0000000
--- a/wandb/run-20241114_074457-qnoibns1/logs/debug-internal.log
+++ /dev/null
@@ -1,18 +0,0 @@
-{"time":"2024-11-14T07:44:57.818065776Z","level":"INFO","msg":"using version","core version":"0.18.7"}
-{"time":"2024-11-14T07:44:57.818079901Z","level":"INFO","msg":"created symlink","path":"wandb/run-20241114_074457-qnoibns1/logs/debug-core.log"}
-{"time":"2024-11-14T07:44:57.926968825Z","level":"INFO","msg":"created new stream","id":"qnoibns1"}
-{"time":"2024-11-14T07:44:57.927012042Z","level":"INFO","msg":"stream: started","id":"qnoibns1"}
-{"time":"2024-11-14T07:44:57.927056482Z","level":"INFO","msg":"writer: Do: started","stream_id":"qnoibns1"}
-{"time":"2024-11-14T07:44:57.92707241Z","level":"INFO","msg":"handler: started","stream_id":"qnoibns1"}
-{"time":"2024-11-14T07:44:57.92709459Z","level":"INFO","msg":"sender: started","stream_id":"qnoibns1"}
-{"time":"2024-11-14T07:44:58.28547417Z","level":"INFO","msg":"Starting system monitor"}
-{"time":"2024-11-14T07:44:58.289289952Z","level":"ERROR","msg":"error generating diff","error":"no diff found"}
-{"time":"2024-11-14T07:44:58.294228632Z","level":"ERROR","msg":"error generating diff","error":"no diff found"}
-{"time":"2024-11-14T07:46:01.542709401Z","level":"INFO","msg":"stream: closing","id":"qnoibns1"}
-{"time":"2024-11-14T07:46:01.542762235Z","level":"INFO","msg":"Stopping system monitor"}
-{"time":"2024-11-14T07:46:01.543631048Z","level":"INFO","msg":"Stopped system monitor"}
-{"time":"2024-11-14T07:46:03.426898119Z","level":"INFO","msg":"fileTransfer: Close: file transfer manager closed"}
-{"time":"2024-11-14T07:46:03.572971036Z","level":"INFO","msg":"handler: closed","stream_id":"qnoibns1"}
-{"time":"2024-11-14T07:46:03.573019172Z","level":"INFO","msg":"writer: Close: closed","stream_id":"qnoibns1"}
-{"time":"2024-11-14T07:46:03.573050047Z","level":"INFO","msg":"sender: closed","stream_id":"qnoibns1"}
-{"time":"2024-11-14T07:46:03.573120673Z","level":"INFO","msg":"stream: closed","id":"qnoibns1"}
diff --git a/wandb/run-20241114_074457-qnoibns1/logs/debug.log b/wandb/run-20241114_074457-qnoibns1/logs/debug.log
deleted file mode 100644
index 5c6c34b..0000000
--- a/wandb/run-20241114_074457-qnoibns1/logs/debug.log
+++ /dev/null
@@ -1,28 +0,0 @@
-2024-11-14 07:44:57,811 INFO    MainThread:685916 [wandb_setup.py:_flush():79] Current SDK version is 0.18.7
-2024-11-14 07:44:57,811 INFO    MainThread:685916 [wandb_setup.py:_flush():79] Configure stats pid to 685916
-2024-11-14 07:44:57,811 INFO    MainThread:685916 [wandb_setup.py:_flush():79] Loading settings from /home/rbhalerao/.config/wandb/settings
-2024-11-14 07:44:57,811 INFO    MainThread:685916 [wandb_setup.py:_flush():79] Loading settings from /scratch/users/rbhalerao/CPH200A_project2/wandb/settings
-2024-11-14 07:44:57,811 INFO    MainThread:685916 [wandb_setup.py:_flush():79] Loading settings from environment variables: {}
-2024-11-14 07:44:57,811 INFO    MainThread:685916 [wandb_setup.py:_flush():79] Applying setup settings: {'mode': None, '_disable_service': None}
-2024-11-14 07:44:57,811 INFO    MainThread:685916 [wandb_setup.py:_flush():79] Inferring run settings from compute environment: {'program_relpath': 'scripts/main.py', 'program_abspath': '/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py', 'program': '/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py'}
-2024-11-14 07:44:57,812 INFO    MainThread:685916 [wandb_setup.py:_flush():79] Applying login settings: {}
-2024-11-14 07:44:57,812 INFO    MainThread:685916 [wandb_setup.py:_flush():79] Applying login settings: {'api_key': '***REDACTED***'}
-2024-11-14 07:44:57,812 INFO    MainThread:685916 [wandb_init.py:_log_setup():533] Logging user logs to ./wandb/run-20241114_074457-qnoibns1/logs/debug.log
-2024-11-14 07:44:57,812 INFO    MainThread:685916 [wandb_init.py:_log_setup():534] Logging internal logs to ./wandb/run-20241114_074457-qnoibns1/logs/debug-internal.log
-2024-11-14 07:44:57,812 INFO    MainThread:685916 [wandb_init.py:init():619] calling init triggers
-2024-11-14 07:44:57,812 INFO    MainThread:685916 [wandb_init.py:init():626] wandb.init called with sweep_config: {}
-config: {}
-2024-11-14 07:44:57,812 INFO    MainThread:685916 [wandb_init.py:init():669] starting backend
-2024-11-14 07:44:57,812 INFO    MainThread:685916 [wandb_init.py:init():673] sending inform_init request
-2024-11-14 07:44:57,815 INFO    MainThread:685916 [backend.py:_multiprocessing_setup():104] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
-2024-11-14 07:44:57,815 INFO    MainThread:685916 [wandb_init.py:init():686] backend started and connected
-2024-11-14 07:44:57,820 INFO    MainThread:685916 [wandb_init.py:init():781] updated telemetry
-2024-11-14 07:44:57,825 INFO    MainThread:685916 [wandb_init.py:init():814] communicating run to backend with 90.0 second timeout
-2024-11-14 07:44:58,281 INFO    MainThread:685916 [wandb_init.py:init():867] starting run threads in backend
-2024-11-14 07:44:58,399 INFO    MainThread:685916 [wandb_run.py:_console_start():2456] atexit reg
-2024-11-14 07:44:58,399 INFO    MainThread:685916 [wandb_run.py:_redirect():2305] redirect: wrap_raw
-2024-11-14 07:44:58,399 INFO    MainThread:685916 [wandb_run.py:_redirect():2370] Wrapping output streams.
-2024-11-14 07:44:58,399 INFO    MainThread:685916 [wandb_run.py:_redirect():2395] Redirects installed.
-2024-11-14 07:44:58,400 INFO    MainThread:685916 [wandb_init.py:init():911] run started, returning control to user process
-2024-11-14 07:45:21,602 INFO    MainThread:685916 [wandb_run.py:_config_callback():1387] config_cb None None {'num_classes': 9, 'init_lr': 0.001, 'pretraining': True, 'num_channels': 3, 'use_data_augmentation': False, 'batch_size': 4, 'num_workers': 14, 'nlst_metadata_path': '/scratch/project2/nlst-metadata/full_nlst_google.json', 'valid_exam_path': '/scratch/project2/nlst-metadata/valid_exams.p', 'nlst_dir': '/scratch/project2/compressed', 'lungrads_path': '/scratch/project2/nlst-metadata/nlst_acc2lungrads.p', 'num_images': 200, 'max_followup': 6, 'img_size': [256, 256], 'class_balance': False, 'group_keys': ['race', 'educat', 'gender', 'age', 'ethnic'], 'feature_config': ['age']}
-2024-11-14 07:46:01,542 WARNING MsgRouterThr:685916 [router.py:message_loop():75] message_loop has been closed
diff --git a/wandb/run-20241114_074457-qnoibns1/run-qnoibns1.wandb b/wandb/run-20241114_074457-qnoibns1/run-qnoibns1.wandb
deleted file mode 100644
index b360adb..0000000
Binary files a/wandb/run-20241114_074457-qnoibns1/run-qnoibns1.wandb and /dev/null differ
diff --git a/wandb/run-20241114_075358-f4xro7um/files/code/scripts/main.py b/wandb/run-20241114_075358-f4xro7um/files/code/scripts/main.py
deleted file mode 100644
index 8ac4d07..0000000
--- a/wandb/run-20241114_075358-f4xro7um/files/code/scripts/main.py
+++ /dev/null
@@ -1,315 +0,0 @@
-import inspect
-import math
-import argparse
-import sys
-import os
-
-sys.path.append(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))
-from src.lightning import MLP, CNN, LinearModel, ResNet18, RiskModel, ResNet3D, Swin3DModel
-from src.dataset import PathMnist, NLST
-from lightning.pytorch.cli import LightningArgumentParser
-import lightning.pytorch as pl
-from torch.cuda import device_count
-import wandb
-from torch.backends import cudnn
-from lightning import seed_everything
-import json
-
-dirname = os.path.dirname(__file__)
-global_seed = json.load(open(os.path.join(dirname, '..', 'global_seed.json')))['global_seed']
-seed_everything(global_seed)
-cudnn.benchmark = False
-
-NAME_TO_MODEL_CLASS = {
-    "mlp": MLP,
-    "cnn": CNN,
-    # "cnn3d": CNN3D,
-    "linear": LinearModel,
-    "resnet": ResNet18,
-    "resnet3d": ResNet3D,
-    "swin3d": Swin3DModel,
-    "risk_model": RiskModel
-}
-
-
-NAME_TO_DATASET_CLASS = {
-    "pathmnist": PathMnist,
-    "nlst": NLST
-}
-
-MODEL_TO_DATASET = {
-    "mlp": "pathmnist",
-    "cnn": "pathmnist",
-    "cnn3d": "nlst",
-    "linear": "pathmnist",
-    "resnet": "pathmnist",
-    "resnet3d": "nlst",
-    "swin3d": "nlst",
-    "risk_model": "nlst"
-}
-
-dirname = os.path.dirname(__file__)
-
-def add_main_args(parser: LightningArgumentParser) -> LightningArgumentParser:
-
-    parser.add_argument(
-        "--model_name",
-        default="mlp",
-        choices=["mlp", "linear", "cnn", "resnet", "risk_model", "swin3d", "resnet3d"],  
-        help="Name of model to use",
-    )
-
-    parser.add_argument(
-        "--dataset_name",
-        default="pathmnist",
-        choices=["pathmnist", "nlst"],
-        help="Name of dataset to use"
-    )
-
-    parser.add_argument(
-        "--project_name",
-        default="cornerstone_project_2",
-        help="Name of project for wandb"
-    )
-
-    parser.add_argument(
-        "--monitor_key",
-        default="val_loss",
-        help="Name of metric to use for checkpointing. (e.g. val_loss, val_acc)"
-    )
-
-    parser.add_argument(
-        "--checkpoint_path",
-        default=None,
-        help="Path to checkpoint to load from. If None, init from scratch."
-    )
-
-    parser.add_argument(
-        "--train",
-        default=False,
-        type=bool,
-        help="Whether to train the model."
-    )
-
-    parser.add_argument(
-        "--num_layers",
-        default=1,
-        type=int,
-        help="Depth of the model (number of layers)",
-    )
-
-    parser.add_argument(
-        "--use_bn",
-        default=False,
-        type=bool,
-        help="Whether to batch normalize in each layer",
-    )
-
-    parser.add_argument(
-        "--hidden_dim",
-        default=512,
-        type=int,
-        help="The dimension of the hidden layer(s)"
-    )
-
-    parser.add_argument(
-        "--use_data_augmentation",
-        default=False,
-        type=bool,
-        help="Whether to augment the data"
-    )
-
-    parser.add_argument(
-        "--pretraining",
-        default=False,
-        type=bool,
-        help="Whether to use pretrained model weights (only used for resnet)"
-    )
-
-    parser.add_argument(
-        "--wandb_entity",
-        default='CPH29',
-        type=str,
-        help="The wandb account to log metrics and models to"
-    )
-
-    parser.add_argument(
-        "--num_workers",
-        type=int,
-        default=1,
-        help="Number of processes to running in parallel"
-    )
-
-    parser.add_argument(
-        "--class_balance",
-        default=False,
-        type=bool,
-        help="Whether to perform class-balanced sampling (only used for nlst dataset)"
-    )
-
-    parser.add_argument(
-        "--batch_size",
-        default=4,
-        type=int,
-        help="Number of samples per batch"
-    )
-
-    parser.add_argument(
-        "--group_keys",
-        default=['race', 'educat', 'gender', 'age', 'ethnic'],
-        nargs='*',
-        help="The groups to perform subgroup analysis on (only used for nlst dataset)"
-    )
-
-    return parser
-
-def parse_args() -> argparse.Namespace:
-    parser = LightningArgumentParser()
-    parser.add_lightning_class_args(pl.Trainer, nested_key="trainer")
-    for model_name, model_class in NAME_TO_MODEL_CLASS.items():
-        parser.add_lightning_class_args(model_class, nested_key=model_name)
-    for dataset_name, data_class in NAME_TO_DATASET_CLASS.items():
-        parser.add_lightning_class_args(data_class, nested_key=dataset_name)
-    parser = add_main_args(parser)
-    args = parser.parse_args()
-    return args
-
-def get_caller():
-    caller = os.path.split(inspect.getsourcefile(sys._getframe(1)))[-1]
-    return caller
-
-def get_datamodule_num_workers(num_process_workers=None):
-    # set per https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813/5
-    num_process_workers = num_process_workers if num_process_workers else 1
-    datamodule_num_workers = device_count() * 8
-    n_cpus = os.cpu_count()
-    if datamodule_num_workers * num_process_workers >= n_cpus:
-        datamodule_num_workers = math.floor(n_cpus/num_process_workers * .9) 
-    return datamodule_num_workers
-
-def get_datamodule(args):
-    # get workers for datamodule
-    datamodule_num_workers = get_datamodule_num_workers(args.num_workers)
-    
-    # get datamodule args
-    datamodule_vars = vars(vars(args)[args.dataset_name])
-    update_vars = {k:v for k,v in vars(args).items() if k in datamodule_vars}
-    datamodule_vars.update(update_vars)
-    datamodule_vars.update({'num_workers': datamodule_num_workers})
-
-    # init data module
-    datamodule = NAME_TO_DATASET_CLASS[args.dataset_name](**datamodule_vars)
-
-    return datamodule
-
-def get_model(args):
-    print(f"Initializing {args.model_name} model")
-    if args.checkpoint_path is None:
-        model_vars = vars(vars(args)[args.model_name])
-        update_vars = {k:v for k,v in vars(args).items() if k in model_vars}
-        model_vars.update(update_vars)
-        print('with params ', model_vars)
-        model = NAME_TO_MODEL_CLASS[args.model_name](**model_vars)
-    else:
-        model = NAME_TO_MODEL_CLASS[args.model_name].load_from_checkpoint(args.checkpoint_path)
-
-    return model
-
-def get_trainer(args, strategy='ddp', logger=None, callbacks=[]):
-    args.trainer.accelerator = 'auto'
-    args.trainer.strategy = strategy
-    args.trainer.logger = logger
-    args.trainer.precision = "bf16-mixed" ## This mixed precision training is highly recommended
-    args.trainer.min_epochs = 100
-
-    # set checkpoint save directory
-    dirpath = os.path.join(dirname, '../models', args.model_name)
-    if not os.path.isdir(dirpath):
-        os.makedirs(dirpath)
-
-    args.trainer.callbacks = callbacks
-
-    # init trainer
-    trainer_args = vars(args.trainer)
-    trainer = pl.Trainer(**trainer_args)
-
-    return trainer
-
-def get_logger(args):
-    logger = pl.loggers.WandbLogger(project=args.project_name,
-                                    entity=args.wandb_entity,
-                                    group=args.model_name,
-                                    dir=os.path.join(dirname, '..'))
-
-    return logger
-
-def get_callbacks(args):
-    # set checkpoint save directory
-    dirpath = os.path.join(dirname, '../models', args.model_name)
-    if not os.path.isdir(dirpath):
-        os.makedirs(dirpath)
-    
-    callbacks = [
-        pl.callbacks.ModelCheckpoint(
-            monitor=args.monitor_key,
-            mode='min' if "loss" in args.monitor_key else "max",
-            dirpath=dirpath,
-            filename=args.model_name + '-{epoch:002d}-{val_loss:.2f}',
-            save_last=True,
-            every_n_epochs=1
-        ),
-        pl.callbacks.EarlyStopping(
-            monitor=args.monitor_key,
-            mode='min' if "loss" in args.monitor_key else "max",
-            patience=10,
-            check_on_train_epoch_end=True
-        )]
-    
-    return callbacks
-
-
-def main(args: argparse.Namespace):
-    print("Loading data ..")
-    print(f"Training: {args.train}")
-    print(f"Model Name: {args.model_name}")
-    print(f"Dataset Name: {args.dataset_name}")
-    print(f"Pretraining: {args.pretraining}")
-    print(f"Num Workers: {args.num_workers}")
-
-    print("Preparing lighning data module (encapsulates dataset init and data loaders)")
-    """
-        Most the data loading logic is pre-implemented in the LightningDataModule class for you.
-        However, you may want to alter this code for special localization logic or to suit your risk
-        model implementations
-    """
-
-    datamodule = get_datamodule(args)
-    model = get_model(args)
-    logger = get_logger(args)
-    callbacks = get_callbacks(args)
-    trainer = get_trainer(args, callbacks=callbacks, logger=logger)
-
-    if args.train:
-        print("Training model")
-        trainer.fit(model, datamodule)
-
-    print("Best model checkpoint path: ", trainer.checkpoint_callback.best_model_path)
-
-    print("Evaluating model on validation set")
-    trainer.validate(model, datamodule)
-
-    print("Evaluating model on test set")
-    trainer.test(model, datamodule)
-
-    if logger:
-        logger.finalize('success')
-    wandb.finish()
-
-    print("Done")
-
-
-if __name__ == '__main__':
-    __spec__ = None
-    args = parse_args()
-    main(args)
-
diff --git a/wandb/run-20241114_075358-f4xro7um/files/config.yaml b/wandb/run-20241114_075358-f4xro7um/files/config.yaml
deleted file mode 100644
index b27a260..0000000
--- a/wandb/run-20241114_075358-f4xro7um/files/config.yaml
+++ /dev/null
@@ -1,80 +0,0 @@
-_wandb:
-    value:
-        cli_version: 0.18.7
-        code_path: code/scripts/main.py
-        m:
-            - "1": trainer/global_step
-              "6":
-                - 3
-              "7": []
-        python_version: 3.10.15
-        t:
-            "1":
-                - 1
-                - 5
-                - 41
-                - 50
-                - 53
-                - 55
-                - 106
-            "2":
-                - 1
-                - 5
-                - 41
-                - 50
-                - 53
-                - 55
-                - 106
-            "3":
-                - 7
-                - 23
-                - 55
-                - 66
-            "4": 3.10.15
-            "5": 0.18.7
-            "8":
-                - 5
-            "12": 0.18.7
-            "13": linux-x86_64
-batch_size:
-    value: 2
-class_balance:
-    value: false
-feature_config:
-    value:
-        - age
-group_keys:
-    value:
-        - race
-        - educat
-        - gender
-        - age
-        - ethnic
-img_size:
-    value:
-        - 256
-        - 256
-init_lr:
-    value: 0.001
-lungrads_path:
-    value: /scratch/project2/nlst-metadata/nlst_acc2lungrads.p
-max_followup:
-    value: 6
-nlst_dir:
-    value: /scratch/project2/compressed
-nlst_metadata_path:
-    value: /scratch/project2/nlst-metadata/full_nlst_google.json
-num_channels:
-    value: 3
-num_classes:
-    value: 9
-num_images:
-    value: 200
-num_workers:
-    value: 14
-pretraining:
-    value: true
-use_data_augmentation:
-    value: false
-valid_exam_path:
-    value: /scratch/project2/nlst-metadata/valid_exams.p
diff --git a/wandb/run-20241114_075358-f4xro7um/files/diff_3eca8d6930aef344f1fe58ce268871cfa453ac07.patch b/wandb/run-20241114_075358-f4xro7um/files/diff_3eca8d6930aef344f1fe58ce268871cfa453ac07.patch
deleted file mode 100644
index b8b20e0..0000000
--- a/wandb/run-20241114_075358-f4xro7um/files/diff_3eca8d6930aef344f1fe58ce268871cfa453ac07.patch
+++ /dev/null
@@ -1,12 +0,0 @@
-diff --git a/src/lightning.py b/src/lightning.py
-index 767a5b6..4f9299a 100644
---- a/src/lightning.py
-+++ b/src/lightning.py
-@@ -333,6 +333,7 @@ class MLP(Classifer):
-         self.model.apply(self.init_weights)
- 
-     def forward(self, x):
-+        print(x.size())
-         batch_size, channels, width, height = x.size()
-         x = rearrange(x, 'b c w h -> b (w h c)')
-         return self.model(x)
diff --git a/wandb/run-20241114_075358-f4xro7um/files/output.log b/wandb/run-20241114_075358-f4xro7um/files/output.log
deleted file mode 100644
index e7f4f89..0000000
--- a/wandb/run-20241114_075358-f4xro7um/files/output.log
+++ /dev/null
@@ -1,120 +0,0 @@
-100%|███████████████████████████████████████████████████████████████████████| 15000/15000 [00:00<00:00, 35287.53it/s]
-NLST Dataset. 28161 exams (417.0 with cancer in one year, 1444 cancer ever) from 9646 patients
-NLST Dataset. 6839 exams (99.0 with cancer in one year, 337 cancer ever) from 2336 patients
-NLST Dataset. 6479 exams (86.0 with cancer in one year, 320 cancer ever) from 2204 patients
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
-
-  | Name       | Type               | Params | Mode
-----------------------------------------------------------
-0 | loss       | CrossEntropyLoss   | 0      | train
-1 | accuracy   | MulticlassAccuracy | 0      | train
-2 | auc        | MulticlassAUROC    | 0      | train
-3 | classifier | ResNet             | 11.2 M | train
-----------------------------------------------------------
-11.2 M    Trainable params
-0         Non-trainable params
-11.2 M    Total params
-44.725    Total estimated model params size (MB)
-71        Modules in train mode
-0         Modules in eval mode
-Sanity Checking DataLoader 0:   0%|                                                            | 0/2 [00:00<?, ?it/s]
-Traceback (most recent call last):
-  File "/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py", line 314, in <module>
-    main(args)
-  File "/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py", line 294, in main
-    trainer.fit(model, datamodule)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
-    call._call_and_handle_interrupt(
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
-    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
-    return function(*args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
-    self._run(model, ckpt_path=ckpt_path)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
-    results = self._run_stage()
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1023, in _run_stage
-    self._run_sanity_check()
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1052, in _run_sanity_check
-    val_loop.run()
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py", line 178, in _decorator
-    return loop_run(self, *args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 135, in run
-    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 396, in _evaluation_step
-    output = call._call_strategy_hook(trainer, hook_name, *step_args)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 319, in _call_strategy_hook
-    output = fn(*args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 410, in validation_step
-    return self._forward_redirection(self.model, self.lightning_module, "validation_step", *args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 640, in __call__
-    wrapper_output = wrapper_module(*args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
-    return self._call_impl(*args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
-    return forward_call(*args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
-    else self._run_ddp_forward(*inputs, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
-    return self.module(*inputs, **kwargs)  # type: ignore[index]
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
-    return self._call_impl(*args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
-    return forward_call(*args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 633, in wrapped_forward
-    out = method(*_args, **_kwargs)
-  File "/scratch/users/rbhalerao/CPH200A_project2/src/lightning.py", line 71, in validation_step
-    y_hat = self.forward(x)
-  File "/scratch/users/rbhalerao/CPH200A_project2/src/lightning.py", line 458, in forward
-    batch_size, channels, width, height = x.size()
-ValueError: too many values to unpack (expected 4)
-[rank0]: Traceback (most recent call last):
-[rank0]:   File "/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py", line 314, in <module>
-[rank0]:     main(args)
-[rank0]:   File "/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py", line 294, in main
-[rank0]:     trainer.fit(model, datamodule)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
-[rank0]:     call._call_and_handle_interrupt(
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
-[rank0]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
-[rank0]:     return function(*args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
-[rank0]:     self._run(model, ckpt_path=ckpt_path)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
-[rank0]:     results = self._run_stage()
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1023, in _run_stage
-[rank0]:     self._run_sanity_check()
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1052, in _run_sanity_check
-[rank0]:     val_loop.run()
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py", line 178, in _decorator
-[rank0]:     return loop_run(self, *args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 135, in run
-[rank0]:     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 396, in _evaluation_step
-[rank0]:     output = call._call_strategy_hook(trainer, hook_name, *step_args)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 319, in _call_strategy_hook
-[rank0]:     output = fn(*args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 410, in validation_step
-[rank0]:     return self._forward_redirection(self.model, self.lightning_module, "validation_step", *args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 640, in __call__
-[rank0]:     wrapper_output = wrapper_module(*args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
-[rank0]:     return self._call_impl(*args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
-[rank0]:     return forward_call(*args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
-[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
-[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
-[rank0]:     return self._call_impl(*args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
-[rank0]:     return forward_call(*args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 633, in wrapped_forward
-[rank0]:     out = method(*_args, **_kwargs)
-[rank0]:   File "/scratch/users/rbhalerao/CPH200A_project2/src/lightning.py", line 71, in validation_step
-[rank0]:     y_hat = self.forward(x)
-[rank0]:   File "/scratch/users/rbhalerao/CPH200A_project2/src/lightning.py", line 458, in forward
-[rank0]:     batch_size, channels, width, height = x.size()
-[rank0]: ValueError: too many values to unpack (expected 4)
diff --git a/wandb/run-20241114_075358-f4xro7um/files/wandb-metadata.json b/wandb/run-20241114_075358-f4xro7um/files/wandb-metadata.json
deleted file mode 100644
index a30a100..0000000
--- a/wandb/run-20241114_075358-f4xro7um/files/wandb-metadata.json
+++ /dev/null
@@ -1,81 +0,0 @@
-{
-  "os": "Linux-6.8.0-1014-nvidia-x86_64-with-glibc2.35",
-  "python": "3.10.15",
-  "startedAt": "2024-11-14T07:53:58.110475Z",
-  "args": [
-    "--model_name",
-    "resnet",
-    "--dataset_name",
-    "nlst",
-    "--train",
-    "True",
-    "--pretraining",
-    "True",
-    "--use_data_augmentation",
-    "False",
-    "--batch_size",
-    "2",
-    "--num_workers",
-    "4",
-    "--project_name",
-    "cornerstone_project_2",
-    "--wandb_entity",
-    "cph29"
-  ],
-  "program": "/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py",
-  "codePath": "scripts/main.py",
-  "git": {
-    "remote": "https://github.com/radhi-bhalerao/CPH200A_project2.git",
-    "commit": "3eca8d6930aef344f1fe58ce268871cfa453ac07"
-  },
-  "email": "rbh@berkeley.edu",
-  "root": ".",
-  "host": "cph-dept-02",
-  "username": "rbhalerao",
-  "executable": "/home/rbhalerao/miniconda3/envs/shree/bin/python",
-  "codePathLocal": "scripts/main.py",
-  "cpu_count": 32,
-  "cpu_count_logical": 64,
-  "gpu": "NVIDIA A40",
-  "gpu_count": 4,
-  "disk": {
-    "/": {
-      "total": "105089261568",
-      "used": "30847610880"
-    }
-  },
-  "memory": {
-    "total": "540682158080"
-  },
-  "cpu": {
-    "count": 32,
-    "countLogical": 64
-  },
-  "gpu_nvidia": [
-    {
-      "name": "NVIDIA A40",
-      "memoryTotal": "48305799168",
-      "cudaCores": 10752,
-      "architecture": "Ampere"
-    },
-    {
-      "name": "NVIDIA A40",
-      "memoryTotal": "48305799168",
-      "cudaCores": 10752,
-      "architecture": "Ampere"
-    },
-    {
-      "name": "NVIDIA A40",
-      "memoryTotal": "48305799168",
-      "cudaCores": 10752,
-      "architecture": "Ampere"
-    },
-    {
-      "name": "NVIDIA A40",
-      "memoryTotal": "48305799168",
-      "cudaCores": 10752,
-      "architecture": "Ampere"
-    }
-  ],
-  "cudaVersion": "12.6"
-}
\ No newline at end of file
diff --git a/wandb/run-20241114_075358-f4xro7um/files/wandb-summary.json b/wandb/run-20241114_075358-f4xro7um/files/wandb-summary.json
deleted file mode 100644
index cfea202..0000000
--- a/wandb/run-20241114_075358-f4xro7um/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"_wandb":{"runtime":42}}
\ No newline at end of file
diff --git a/wandb/run-20241114_075358-f4xro7um/logs/debug-core.log b/wandb/run-20241114_075358-f4xro7um/logs/debug-core.log
deleted file mode 120000
index 0c5c36f..0000000
--- a/wandb/run-20241114_075358-f4xro7um/logs/debug-core.log
+++ /dev/null
@@ -1 +0,0 @@
-/home/rbhalerao/.cache/wandb/logs/core-debug-20241114_075357.log
\ No newline at end of file
diff --git a/wandb/run-20241114_075358-f4xro7um/logs/debug-internal.log b/wandb/run-20241114_075358-f4xro7um/logs/debug-internal.log
deleted file mode 100644
index 9213268..0000000
--- a/wandb/run-20241114_075358-f4xro7um/logs/debug-internal.log
+++ /dev/null
@@ -1,16 +0,0 @@
-{"time":"2024-11-14T07:53:58.113298577Z","level":"INFO","msg":"using version","core version":"0.18.7"}
-{"time":"2024-11-14T07:53:58.113327091Z","level":"INFO","msg":"created symlink","path":"wandb/run-20241114_075358-f4xro7um/logs/debug-core.log"}
-{"time":"2024-11-14T07:53:58.222838635Z","level":"INFO","msg":"created new stream","id":"f4xro7um"}
-{"time":"2024-11-14T07:53:58.222876136Z","level":"INFO","msg":"stream: started","id":"f4xro7um"}
-{"time":"2024-11-14T07:53:58.222948193Z","level":"INFO","msg":"writer: Do: started","stream_id":"f4xro7um"}
-{"time":"2024-11-14T07:53:58.223221592Z","level":"INFO","msg":"handler: started","stream_id":"f4xro7um"}
-{"time":"2024-11-14T07:53:58.223247952Z","level":"INFO","msg":"sender: started","stream_id":"f4xro7um"}
-{"time":"2024-11-14T07:53:58.584435477Z","level":"INFO","msg":"Starting system monitor"}
-{"time":"2024-11-14T07:54:40.672394656Z","level":"INFO","msg":"stream: closing","id":"f4xro7um"}
-{"time":"2024-11-14T07:54:40.672421086Z","level":"INFO","msg":"Stopping system monitor"}
-{"time":"2024-11-14T07:54:40.673003808Z","level":"INFO","msg":"Stopped system monitor"}
-{"time":"2024-11-14T07:54:43.492290945Z","level":"INFO","msg":"fileTransfer: Close: file transfer manager closed"}
-{"time":"2024-11-14T07:54:43.719475126Z","level":"INFO","msg":"handler: closed","stream_id":"f4xro7um"}
-{"time":"2024-11-14T07:54:43.719559706Z","level":"INFO","msg":"writer: Close: closed","stream_id":"f4xro7um"}
-{"time":"2024-11-14T07:54:43.719574724Z","level":"INFO","msg":"sender: closed","stream_id":"f4xro7um"}
-{"time":"2024-11-14T07:54:43.719701654Z","level":"INFO","msg":"stream: closed","id":"f4xro7um"}
diff --git a/wandb/run-20241114_075358-f4xro7um/logs/debug.log b/wandb/run-20241114_075358-f4xro7um/logs/debug.log
deleted file mode 100644
index 8e92cfe..0000000
--- a/wandb/run-20241114_075358-f4xro7um/logs/debug.log
+++ /dev/null
@@ -1,27 +0,0 @@
-2024-11-14 07:53:58,105 INFO    MainThread:743256 [wandb_setup.py:_flush():79] Current SDK version is 0.18.7
-2024-11-14 07:53:58,105 INFO    MainThread:743256 [wandb_setup.py:_flush():79] Configure stats pid to 743256
-2024-11-14 07:53:58,105 INFO    MainThread:743256 [wandb_setup.py:_flush():79] Loading settings from /home/rbhalerao/.config/wandb/settings
-2024-11-14 07:53:58,105 INFO    MainThread:743256 [wandb_setup.py:_flush():79] Loading settings from /scratch/users/rbhalerao/CPH200A_project2/wandb/settings
-2024-11-14 07:53:58,105 INFO    MainThread:743256 [wandb_setup.py:_flush():79] Loading settings from environment variables: {}
-2024-11-14 07:53:58,105 INFO    MainThread:743256 [wandb_setup.py:_flush():79] Applying setup settings: {'mode': None, '_disable_service': None}
-2024-11-14 07:53:58,105 INFO    MainThread:743256 [wandb_setup.py:_flush():79] Inferring run settings from compute environment: {'program_relpath': 'scripts/main.py', 'program_abspath': '/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py', 'program': '/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py'}
-2024-11-14 07:53:58,105 INFO    MainThread:743256 [wandb_setup.py:_flush():79] Applying login settings: {}
-2024-11-14 07:53:58,105 INFO    MainThread:743256 [wandb_init.py:_log_setup():533] Logging user logs to ./wandb/run-20241114_075358-f4xro7um/logs/debug.log
-2024-11-14 07:53:58,106 INFO    MainThread:743256 [wandb_init.py:_log_setup():534] Logging internal logs to ./wandb/run-20241114_075358-f4xro7um/logs/debug-internal.log
-2024-11-14 07:53:58,106 INFO    MainThread:743256 [wandb_init.py:init():619] calling init triggers
-2024-11-14 07:53:58,106 INFO    MainThread:743256 [wandb_init.py:init():626] wandb.init called with sweep_config: {}
-config: {}
-2024-11-14 07:53:58,106 INFO    MainThread:743256 [wandb_init.py:init():669] starting backend
-2024-11-14 07:53:58,106 INFO    MainThread:743256 [wandb_init.py:init():673] sending inform_init request
-2024-11-14 07:53:58,109 INFO    MainThread:743256 [backend.py:_multiprocessing_setup():104] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
-2024-11-14 07:53:58,110 INFO    MainThread:743256 [wandb_init.py:init():686] backend started and connected
-2024-11-14 07:53:58,117 INFO    MainThread:743256 [wandb_init.py:init():781] updated telemetry
-2024-11-14 07:53:58,122 INFO    MainThread:743256 [wandb_init.py:init():814] communicating run to backend with 90.0 second timeout
-2024-11-14 07:53:58,578 INFO    MainThread:743256 [wandb_init.py:init():867] starting run threads in backend
-2024-11-14 07:53:58,690 INFO    MainThread:743256 [wandb_run.py:_console_start():2456] atexit reg
-2024-11-14 07:53:58,690 INFO    MainThread:743256 [wandb_run.py:_redirect():2305] redirect: wrap_raw
-2024-11-14 07:53:58,690 INFO    MainThread:743256 [wandb_run.py:_redirect():2370] Wrapping output streams.
-2024-11-14 07:53:58,690 INFO    MainThread:743256 [wandb_run.py:_redirect():2395] Redirects installed.
-2024-11-14 07:53:58,691 INFO    MainThread:743256 [wandb_init.py:init():911] run started, returning control to user process
-2024-11-14 07:54:22,014 INFO    MainThread:743256 [wandb_run.py:_config_callback():1387] config_cb None None {'num_classes': 9, 'init_lr': 0.001, 'pretraining': True, 'num_channels': 3, 'use_data_augmentation': False, 'batch_size': 2, 'num_workers': 14, 'nlst_metadata_path': '/scratch/project2/nlst-metadata/full_nlst_google.json', 'valid_exam_path': '/scratch/project2/nlst-metadata/valid_exams.p', 'nlst_dir': '/scratch/project2/compressed', 'lungrads_path': '/scratch/project2/nlst-metadata/nlst_acc2lungrads.p', 'num_images': 200, 'max_followup': 6, 'img_size': [256, 256], 'class_balance': False, 'group_keys': ['race', 'educat', 'gender', 'age', 'ethnic'], 'feature_config': ['age']}
-2024-11-14 07:54:40,672 WARNING MsgRouterThr:743256 [router.py:message_loop():75] message_loop has been closed
diff --git a/wandb/run-20241114_075358-f4xro7um/run-f4xro7um.wandb b/wandb/run-20241114_075358-f4xro7um/run-f4xro7um.wandb
deleted file mode 100644
index 62be6be..0000000
Binary files a/wandb/run-20241114_075358-f4xro7um/run-f4xro7um.wandb and /dev/null differ
diff --git a/wandb/run-20241114_075528-gai09q5t/files/code/scripts/main.py b/wandb/run-20241114_075528-gai09q5t/files/code/scripts/main.py
deleted file mode 100644
index 8ac4d07..0000000
--- a/wandb/run-20241114_075528-gai09q5t/files/code/scripts/main.py
+++ /dev/null
@@ -1,315 +0,0 @@
-import inspect
-import math
-import argparse
-import sys
-import os
-
-sys.path.append(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))
-from src.lightning import MLP, CNN, LinearModel, ResNet18, RiskModel, ResNet3D, Swin3DModel
-from src.dataset import PathMnist, NLST
-from lightning.pytorch.cli import LightningArgumentParser
-import lightning.pytorch as pl
-from torch.cuda import device_count
-import wandb
-from torch.backends import cudnn
-from lightning import seed_everything
-import json
-
-dirname = os.path.dirname(__file__)
-global_seed = json.load(open(os.path.join(dirname, '..', 'global_seed.json')))['global_seed']
-seed_everything(global_seed)
-cudnn.benchmark = False
-
-NAME_TO_MODEL_CLASS = {
-    "mlp": MLP,
-    "cnn": CNN,
-    # "cnn3d": CNN3D,
-    "linear": LinearModel,
-    "resnet": ResNet18,
-    "resnet3d": ResNet3D,
-    "swin3d": Swin3DModel,
-    "risk_model": RiskModel
-}
-
-
-NAME_TO_DATASET_CLASS = {
-    "pathmnist": PathMnist,
-    "nlst": NLST
-}
-
-MODEL_TO_DATASET = {
-    "mlp": "pathmnist",
-    "cnn": "pathmnist",
-    "cnn3d": "nlst",
-    "linear": "pathmnist",
-    "resnet": "pathmnist",
-    "resnet3d": "nlst",
-    "swin3d": "nlst",
-    "risk_model": "nlst"
-}
-
-dirname = os.path.dirname(__file__)
-
-def add_main_args(parser: LightningArgumentParser) -> LightningArgumentParser:
-
-    parser.add_argument(
-        "--model_name",
-        default="mlp",
-        choices=["mlp", "linear", "cnn", "resnet", "risk_model", "swin3d", "resnet3d"],  
-        help="Name of model to use",
-    )
-
-    parser.add_argument(
-        "--dataset_name",
-        default="pathmnist",
-        choices=["pathmnist", "nlst"],
-        help="Name of dataset to use"
-    )
-
-    parser.add_argument(
-        "--project_name",
-        default="cornerstone_project_2",
-        help="Name of project for wandb"
-    )
-
-    parser.add_argument(
-        "--monitor_key",
-        default="val_loss",
-        help="Name of metric to use for checkpointing. (e.g. val_loss, val_acc)"
-    )
-
-    parser.add_argument(
-        "--checkpoint_path",
-        default=None,
-        help="Path to checkpoint to load from. If None, init from scratch."
-    )
-
-    parser.add_argument(
-        "--train",
-        default=False,
-        type=bool,
-        help="Whether to train the model."
-    )
-
-    parser.add_argument(
-        "--num_layers",
-        default=1,
-        type=int,
-        help="Depth of the model (number of layers)",
-    )
-
-    parser.add_argument(
-        "--use_bn",
-        default=False,
-        type=bool,
-        help="Whether to batch normalize in each layer",
-    )
-
-    parser.add_argument(
-        "--hidden_dim",
-        default=512,
-        type=int,
-        help="The dimension of the hidden layer(s)"
-    )
-
-    parser.add_argument(
-        "--use_data_augmentation",
-        default=False,
-        type=bool,
-        help="Whether to augment the data"
-    )
-
-    parser.add_argument(
-        "--pretraining",
-        default=False,
-        type=bool,
-        help="Whether to use pretrained model weights (only used for resnet)"
-    )
-
-    parser.add_argument(
-        "--wandb_entity",
-        default='CPH29',
-        type=str,
-        help="The wandb account to log metrics and models to"
-    )
-
-    parser.add_argument(
-        "--num_workers",
-        type=int,
-        default=1,
-        help="Number of processes to running in parallel"
-    )
-
-    parser.add_argument(
-        "--class_balance",
-        default=False,
-        type=bool,
-        help="Whether to perform class-balanced sampling (only used for nlst dataset)"
-    )
-
-    parser.add_argument(
-        "--batch_size",
-        default=4,
-        type=int,
-        help="Number of samples per batch"
-    )
-
-    parser.add_argument(
-        "--group_keys",
-        default=['race', 'educat', 'gender', 'age', 'ethnic'],
-        nargs='*',
-        help="The groups to perform subgroup analysis on (only used for nlst dataset)"
-    )
-
-    return parser
-
-def parse_args() -> argparse.Namespace:
-    parser = LightningArgumentParser()
-    parser.add_lightning_class_args(pl.Trainer, nested_key="trainer")
-    for model_name, model_class in NAME_TO_MODEL_CLASS.items():
-        parser.add_lightning_class_args(model_class, nested_key=model_name)
-    for dataset_name, data_class in NAME_TO_DATASET_CLASS.items():
-        parser.add_lightning_class_args(data_class, nested_key=dataset_name)
-    parser = add_main_args(parser)
-    args = parser.parse_args()
-    return args
-
-def get_caller():
-    caller = os.path.split(inspect.getsourcefile(sys._getframe(1)))[-1]
-    return caller
-
-def get_datamodule_num_workers(num_process_workers=None):
-    # set per https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813/5
-    num_process_workers = num_process_workers if num_process_workers else 1
-    datamodule_num_workers = device_count() * 8
-    n_cpus = os.cpu_count()
-    if datamodule_num_workers * num_process_workers >= n_cpus:
-        datamodule_num_workers = math.floor(n_cpus/num_process_workers * .9) 
-    return datamodule_num_workers
-
-def get_datamodule(args):
-    # get workers for datamodule
-    datamodule_num_workers = get_datamodule_num_workers(args.num_workers)
-    
-    # get datamodule args
-    datamodule_vars = vars(vars(args)[args.dataset_name])
-    update_vars = {k:v for k,v in vars(args).items() if k in datamodule_vars}
-    datamodule_vars.update(update_vars)
-    datamodule_vars.update({'num_workers': datamodule_num_workers})
-
-    # init data module
-    datamodule = NAME_TO_DATASET_CLASS[args.dataset_name](**datamodule_vars)
-
-    return datamodule
-
-def get_model(args):
-    print(f"Initializing {args.model_name} model")
-    if args.checkpoint_path is None:
-        model_vars = vars(vars(args)[args.model_name])
-        update_vars = {k:v for k,v in vars(args).items() if k in model_vars}
-        model_vars.update(update_vars)
-        print('with params ', model_vars)
-        model = NAME_TO_MODEL_CLASS[args.model_name](**model_vars)
-    else:
-        model = NAME_TO_MODEL_CLASS[args.model_name].load_from_checkpoint(args.checkpoint_path)
-
-    return model
-
-def get_trainer(args, strategy='ddp', logger=None, callbacks=[]):
-    args.trainer.accelerator = 'auto'
-    args.trainer.strategy = strategy
-    args.trainer.logger = logger
-    args.trainer.precision = "bf16-mixed" ## This mixed precision training is highly recommended
-    args.trainer.min_epochs = 100
-
-    # set checkpoint save directory
-    dirpath = os.path.join(dirname, '../models', args.model_name)
-    if not os.path.isdir(dirpath):
-        os.makedirs(dirpath)
-
-    args.trainer.callbacks = callbacks
-
-    # init trainer
-    trainer_args = vars(args.trainer)
-    trainer = pl.Trainer(**trainer_args)
-
-    return trainer
-
-def get_logger(args):
-    logger = pl.loggers.WandbLogger(project=args.project_name,
-                                    entity=args.wandb_entity,
-                                    group=args.model_name,
-                                    dir=os.path.join(dirname, '..'))
-
-    return logger
-
-def get_callbacks(args):
-    # set checkpoint save directory
-    dirpath = os.path.join(dirname, '../models', args.model_name)
-    if not os.path.isdir(dirpath):
-        os.makedirs(dirpath)
-    
-    callbacks = [
-        pl.callbacks.ModelCheckpoint(
-            monitor=args.monitor_key,
-            mode='min' if "loss" in args.monitor_key else "max",
-            dirpath=dirpath,
-            filename=args.model_name + '-{epoch:002d}-{val_loss:.2f}',
-            save_last=True,
-            every_n_epochs=1
-        ),
-        pl.callbacks.EarlyStopping(
-            monitor=args.monitor_key,
-            mode='min' if "loss" in args.monitor_key else "max",
-            patience=10,
-            check_on_train_epoch_end=True
-        )]
-    
-    return callbacks
-
-
-def main(args: argparse.Namespace):
-    print("Loading data ..")
-    print(f"Training: {args.train}")
-    print(f"Model Name: {args.model_name}")
-    print(f"Dataset Name: {args.dataset_name}")
-    print(f"Pretraining: {args.pretraining}")
-    print(f"Num Workers: {args.num_workers}")
-
-    print("Preparing lighning data module (encapsulates dataset init and data loaders)")
-    """
-        Most the data loading logic is pre-implemented in the LightningDataModule class for you.
-        However, you may want to alter this code for special localization logic or to suit your risk
-        model implementations
-    """
-
-    datamodule = get_datamodule(args)
-    model = get_model(args)
-    logger = get_logger(args)
-    callbacks = get_callbacks(args)
-    trainer = get_trainer(args, callbacks=callbacks, logger=logger)
-
-    if args.train:
-        print("Training model")
-        trainer.fit(model, datamodule)
-
-    print("Best model checkpoint path: ", trainer.checkpoint_callback.best_model_path)
-
-    print("Evaluating model on validation set")
-    trainer.validate(model, datamodule)
-
-    print("Evaluating model on test set")
-    trainer.test(model, datamodule)
-
-    if logger:
-        logger.finalize('success')
-    wandb.finish()
-
-    print("Done")
-
-
-if __name__ == '__main__':
-    __spec__ = None
-    args = parse_args()
-    main(args)
-
diff --git a/wandb/run-20241114_075528-gai09q5t/files/config.yaml b/wandb/run-20241114_075528-gai09q5t/files/config.yaml
deleted file mode 100644
index b27a260..0000000
--- a/wandb/run-20241114_075528-gai09q5t/files/config.yaml
+++ /dev/null
@@ -1,80 +0,0 @@
-_wandb:
-    value:
-        cli_version: 0.18.7
-        code_path: code/scripts/main.py
-        m:
-            - "1": trainer/global_step
-              "6":
-                - 3
-              "7": []
-        python_version: 3.10.15
-        t:
-            "1":
-                - 1
-                - 5
-                - 41
-                - 50
-                - 53
-                - 55
-                - 106
-            "2":
-                - 1
-                - 5
-                - 41
-                - 50
-                - 53
-                - 55
-                - 106
-            "3":
-                - 7
-                - 23
-                - 55
-                - 66
-            "4": 3.10.15
-            "5": 0.18.7
-            "8":
-                - 5
-            "12": 0.18.7
-            "13": linux-x86_64
-batch_size:
-    value: 2
-class_balance:
-    value: false
-feature_config:
-    value:
-        - age
-group_keys:
-    value:
-        - race
-        - educat
-        - gender
-        - age
-        - ethnic
-img_size:
-    value:
-        - 256
-        - 256
-init_lr:
-    value: 0.001
-lungrads_path:
-    value: /scratch/project2/nlst-metadata/nlst_acc2lungrads.p
-max_followup:
-    value: 6
-nlst_dir:
-    value: /scratch/project2/compressed
-nlst_metadata_path:
-    value: /scratch/project2/nlst-metadata/full_nlst_google.json
-num_channels:
-    value: 3
-num_classes:
-    value: 9
-num_images:
-    value: 200
-num_workers:
-    value: 14
-pretraining:
-    value: true
-use_data_augmentation:
-    value: false
-valid_exam_path:
-    value: /scratch/project2/nlst-metadata/valid_exams.p
diff --git a/wandb/run-20241114_075528-gai09q5t/files/diff_3eca8d6930aef344f1fe58ce268871cfa453ac07.patch b/wandb/run-20241114_075528-gai09q5t/files/diff_3eca8d6930aef344f1fe58ce268871cfa453ac07.patch
deleted file mode 100644
index a758ff2..0000000
--- a/wandb/run-20241114_075528-gai09q5t/files/diff_3eca8d6930aef344f1fe58ce268871cfa453ac07.patch
+++ /dev/null
@@ -1,12 +0,0 @@
-diff --git a/src/lightning.py b/src/lightning.py
-index 767a5b6..968daae 100644
---- a/src/lightning.py
-+++ b/src/lightning.py
-@@ -333,6 +333,7 @@ class MLP(Classifer):
-         self.model.apply(self.init_weights)
- 
-     def forward(self, x):
-+        print('Size: ', x.size())
-         batch_size, channels, width, height = x.size()
-         x = rearrange(x, 'b c w h -> b (w h c)')
-         return self.model(x)
diff --git a/wandb/run-20241114_075528-gai09q5t/files/output.log b/wandb/run-20241114_075528-gai09q5t/files/output.log
deleted file mode 100644
index c684fc7..0000000
--- a/wandb/run-20241114_075528-gai09q5t/files/output.log
+++ /dev/null
@@ -1,120 +0,0 @@
-100%|███████████████████████████████████████████████████████████████████████| 15000/15000 [00:00<00:00, 36047.06it/s]
-NLST Dataset. 28161 exams (417.0 with cancer in one year, 1444 cancer ever) from 9646 patients
-NLST Dataset. 6839 exams (99.0 with cancer in one year, 337 cancer ever) from 2336 patients
-NLST Dataset. 6479 exams (86.0 with cancer in one year, 320 cancer ever) from 2204 patients
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
-
-  | Name       | Type               | Params | Mode
-----------------------------------------------------------
-0 | loss       | CrossEntropyLoss   | 0      | train
-1 | accuracy   | MulticlassAccuracy | 0      | train
-2 | auc        | MulticlassAUROC    | 0      | train
-3 | classifier | ResNet             | 11.2 M | train
-----------------------------------------------------------
-11.2 M    Trainable params
-0         Non-trainable params
-11.2 M    Total params
-44.725    Total estimated model params size (MB)
-71        Modules in train mode
-0         Modules in eval mode
-Sanity Checking DataLoader 0:   0%|                                                            | 0/2 [00:00<?, ?it/s]
-Traceback (most recent call last):
-  File "/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py", line 314, in <module>
-    main(args)
-  File "/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py", line 294, in main
-    trainer.fit(model, datamodule)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
-    call._call_and_handle_interrupt(
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
-    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
-    return function(*args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
-    self._run(model, ckpt_path=ckpt_path)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
-    results = self._run_stage()
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1023, in _run_stage
-    self._run_sanity_check()
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1052, in _run_sanity_check
-    val_loop.run()
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py", line 178, in _decorator
-    return loop_run(self, *args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 135, in run
-    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 396, in _evaluation_step
-    output = call._call_strategy_hook(trainer, hook_name, *step_args)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 319, in _call_strategy_hook
-    output = fn(*args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 410, in validation_step
-    return self._forward_redirection(self.model, self.lightning_module, "validation_step", *args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 640, in __call__
-    wrapper_output = wrapper_module(*args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
-    return self._call_impl(*args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
-    return forward_call(*args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
-    else self._run_ddp_forward(*inputs, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
-    return self.module(*inputs, **kwargs)  # type: ignore[index]
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
-    return self._call_impl(*args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
-    return forward_call(*args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 633, in wrapped_forward
-    out = method(*_args, **_kwargs)
-  File "/scratch/users/rbhalerao/CPH200A_project2/src/lightning.py", line 71, in validation_step
-    y_hat = self.forward(x)
-  File "/scratch/users/rbhalerao/CPH200A_project2/src/lightning.py", line 458, in forward
-    batch_size, channels, width, height = x.size()
-ValueError: too many values to unpack (expected 4)
-[rank0]: Traceback (most recent call last):
-[rank0]:   File "/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py", line 314, in <module>
-[rank0]:     main(args)
-[rank0]:   File "/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py", line 294, in main
-[rank0]:     trainer.fit(model, datamodule)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
-[rank0]:     call._call_and_handle_interrupt(
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
-[rank0]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
-[rank0]:     return function(*args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
-[rank0]:     self._run(model, ckpt_path=ckpt_path)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
-[rank0]:     results = self._run_stage()
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1023, in _run_stage
-[rank0]:     self._run_sanity_check()
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1052, in _run_sanity_check
-[rank0]:     val_loop.run()
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py", line 178, in _decorator
-[rank0]:     return loop_run(self, *args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 135, in run
-[rank0]:     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 396, in _evaluation_step
-[rank0]:     output = call._call_strategy_hook(trainer, hook_name, *step_args)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 319, in _call_strategy_hook
-[rank0]:     output = fn(*args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 410, in validation_step
-[rank0]:     return self._forward_redirection(self.model, self.lightning_module, "validation_step", *args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 640, in __call__
-[rank0]:     wrapper_output = wrapper_module(*args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
-[rank0]:     return self._call_impl(*args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
-[rank0]:     return forward_call(*args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
-[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
-[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
-[rank0]:     return self._call_impl(*args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
-[rank0]:     return forward_call(*args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 633, in wrapped_forward
-[rank0]:     out = method(*_args, **_kwargs)
-[rank0]:   File "/scratch/users/rbhalerao/CPH200A_project2/src/lightning.py", line 71, in validation_step
-[rank0]:     y_hat = self.forward(x)
-[rank0]:   File "/scratch/users/rbhalerao/CPH200A_project2/src/lightning.py", line 458, in forward
-[rank0]:     batch_size, channels, width, height = x.size()
-[rank0]: ValueError: too many values to unpack (expected 4)
diff --git a/wandb/run-20241114_075528-gai09q5t/files/wandb-metadata.json b/wandb/run-20241114_075528-gai09q5t/files/wandb-metadata.json
deleted file mode 100644
index d6bedac..0000000
--- a/wandb/run-20241114_075528-gai09q5t/files/wandb-metadata.json
+++ /dev/null
@@ -1,81 +0,0 @@
-{
-  "os": "Linux-6.8.0-1014-nvidia-x86_64-with-glibc2.35",
-  "python": "3.10.15",
-  "startedAt": "2024-11-14T07:55:28.479284Z",
-  "args": [
-    "--model_name",
-    "resnet",
-    "--dataset_name",
-    "nlst",
-    "--train",
-    "True",
-    "--pretraining",
-    "True",
-    "--use_data_augmentation",
-    "False",
-    "--batch_size",
-    "2",
-    "--num_workers",
-    "4",
-    "--project_name",
-    "cornerstone_project_2",
-    "--wandb_entity",
-    "cph29"
-  ],
-  "program": "/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py",
-  "codePath": "scripts/main.py",
-  "git": {
-    "remote": "https://github.com/radhi-bhalerao/CPH200A_project2.git",
-    "commit": "3eca8d6930aef344f1fe58ce268871cfa453ac07"
-  },
-  "email": "rbh@berkeley.edu",
-  "root": ".",
-  "host": "cph-dept-02",
-  "username": "rbhalerao",
-  "executable": "/home/rbhalerao/miniconda3/envs/shree/bin/python",
-  "codePathLocal": "scripts/main.py",
-  "cpu_count": 32,
-  "cpu_count_logical": 64,
-  "gpu": "NVIDIA A40",
-  "gpu_count": 4,
-  "disk": {
-    "/": {
-      "total": "105089261568",
-      "used": "30847721472"
-    }
-  },
-  "memory": {
-    "total": "540682158080"
-  },
-  "cpu": {
-    "count": 32,
-    "countLogical": 64
-  },
-  "gpu_nvidia": [
-    {
-      "name": "NVIDIA A40",
-      "memoryTotal": "48305799168",
-      "cudaCores": 10752,
-      "architecture": "Ampere"
-    },
-    {
-      "name": "NVIDIA A40",
-      "memoryTotal": "48305799168",
-      "cudaCores": 10752,
-      "architecture": "Ampere"
-    },
-    {
-      "name": "NVIDIA A40",
-      "memoryTotal": "48305799168",
-      "cudaCores": 10752,
-      "architecture": "Ampere"
-    },
-    {
-      "name": "NVIDIA A40",
-      "memoryTotal": "48305799168",
-      "cudaCores": 10752,
-      "architecture": "Ampere"
-    }
-  ],
-  "cudaVersion": "12.6"
-}
\ No newline at end of file
diff --git a/wandb/run-20241114_075528-gai09q5t/files/wandb-summary.json b/wandb/run-20241114_075528-gai09q5t/files/wandb-summary.json
deleted file mode 100644
index 842300b..0000000
--- a/wandb/run-20241114_075528-gai09q5t/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"_wandb":{"runtime":43}}
\ No newline at end of file
diff --git a/wandb/run-20241114_075528-gai09q5t/logs/debug-core.log b/wandb/run-20241114_075528-gai09q5t/logs/debug-core.log
deleted file mode 120000
index 0a38505..0000000
--- a/wandb/run-20241114_075528-gai09q5t/logs/debug-core.log
+++ /dev/null
@@ -1 +0,0 @@
-/home/rbhalerao/.cache/wandb/logs/core-debug-20241114_075528.log
\ No newline at end of file
diff --git a/wandb/run-20241114_075528-gai09q5t/logs/debug-internal.log b/wandb/run-20241114_075528-gai09q5t/logs/debug-internal.log
deleted file mode 100644
index ced8487..0000000
--- a/wandb/run-20241114_075528-gai09q5t/logs/debug-internal.log
+++ /dev/null
@@ -1,16 +0,0 @@
-{"time":"2024-11-14T07:55:28.482121431Z","level":"INFO","msg":"using version","core version":"0.18.7"}
-{"time":"2024-11-14T07:55:28.482136108Z","level":"INFO","msg":"created symlink","path":"wandb/run-20241114_075528-gai09q5t/logs/debug-core.log"}
-{"time":"2024-11-14T07:55:28.591181407Z","level":"INFO","msg":"created new stream","id":"gai09q5t"}
-{"time":"2024-11-14T07:55:28.591220441Z","level":"INFO","msg":"stream: started","id":"gai09q5t"}
-{"time":"2024-11-14T07:55:28.591276546Z","level":"INFO","msg":"writer: Do: started","stream_id":"gai09q5t"}
-{"time":"2024-11-14T07:55:28.591312304Z","level":"INFO","msg":"handler: started","stream_id":"gai09q5t"}
-{"time":"2024-11-14T07:55:28.591397684Z","level":"INFO","msg":"sender: started","stream_id":"gai09q5t"}
-{"time":"2024-11-14T07:55:29.13056534Z","level":"INFO","msg":"Starting system monitor"}
-{"time":"2024-11-14T07:56:12.054557894Z","level":"INFO","msg":"stream: closing","id":"gai09q5t"}
-{"time":"2024-11-14T07:56:12.054627545Z","level":"INFO","msg":"Stopping system monitor"}
-{"time":"2024-11-14T07:56:12.055185981Z","level":"INFO","msg":"Stopped system monitor"}
-{"time":"2024-11-14T07:56:13.578055217Z","level":"INFO","msg":"fileTransfer: Close: file transfer manager closed"}
-{"time":"2024-11-14T07:56:13.702507319Z","level":"INFO","msg":"handler: closed","stream_id":"gai09q5t"}
-{"time":"2024-11-14T07:56:13.702558905Z","level":"INFO","msg":"writer: Close: closed","stream_id":"gai09q5t"}
-{"time":"2024-11-14T07:56:13.702578462Z","level":"INFO","msg":"sender: closed","stream_id":"gai09q5t"}
-{"time":"2024-11-14T07:56:13.70262518Z","level":"INFO","msg":"stream: closed","id":"gai09q5t"}
diff --git a/wandb/run-20241114_075528-gai09q5t/logs/debug.log b/wandb/run-20241114_075528-gai09q5t/logs/debug.log
deleted file mode 100644
index bfb7ae2..0000000
--- a/wandb/run-20241114_075528-gai09q5t/logs/debug.log
+++ /dev/null
@@ -1,27 +0,0 @@
-2024-11-14 07:55:28,474 INFO    MainThread:770138 [wandb_setup.py:_flush():79] Current SDK version is 0.18.7
-2024-11-14 07:55:28,474 INFO    MainThread:770138 [wandb_setup.py:_flush():79] Configure stats pid to 770138
-2024-11-14 07:55:28,474 INFO    MainThread:770138 [wandb_setup.py:_flush():79] Loading settings from /home/rbhalerao/.config/wandb/settings
-2024-11-14 07:55:28,474 INFO    MainThread:770138 [wandb_setup.py:_flush():79] Loading settings from /scratch/users/rbhalerao/CPH200A_project2/wandb/settings
-2024-11-14 07:55:28,474 INFO    MainThread:770138 [wandb_setup.py:_flush():79] Loading settings from environment variables: {}
-2024-11-14 07:55:28,474 INFO    MainThread:770138 [wandb_setup.py:_flush():79] Applying setup settings: {'mode': None, '_disable_service': None}
-2024-11-14 07:55:28,474 INFO    MainThread:770138 [wandb_setup.py:_flush():79] Inferring run settings from compute environment: {'program_relpath': 'scripts/main.py', 'program_abspath': '/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py', 'program': '/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py'}
-2024-11-14 07:55:28,474 INFO    MainThread:770138 [wandb_setup.py:_flush():79] Applying login settings: {}
-2024-11-14 07:55:28,474 INFO    MainThread:770138 [wandb_init.py:_log_setup():533] Logging user logs to ./wandb/run-20241114_075528-gai09q5t/logs/debug.log
-2024-11-14 07:55:28,474 INFO    MainThread:770138 [wandb_init.py:_log_setup():534] Logging internal logs to ./wandb/run-20241114_075528-gai09q5t/logs/debug-internal.log
-2024-11-14 07:55:28,475 INFO    MainThread:770138 [wandb_init.py:init():619] calling init triggers
-2024-11-14 07:55:28,475 INFO    MainThread:770138 [wandb_init.py:init():626] wandb.init called with sweep_config: {}
-config: {}
-2024-11-14 07:55:28,475 INFO    MainThread:770138 [wandb_init.py:init():669] starting backend
-2024-11-14 07:55:28,475 INFO    MainThread:770138 [wandb_init.py:init():673] sending inform_init request
-2024-11-14 07:55:28,478 INFO    MainThread:770138 [backend.py:_multiprocessing_setup():104] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
-2024-11-14 07:55:28,479 INFO    MainThread:770138 [wandb_init.py:init():686] backend started and connected
-2024-11-14 07:55:28,486 INFO    MainThread:770138 [wandb_init.py:init():781] updated telemetry
-2024-11-14 07:55:28,490 INFO    MainThread:770138 [wandb_init.py:init():814] communicating run to backend with 90.0 second timeout
-2024-11-14 07:55:29,124 INFO    MainThread:770138 [wandb_init.py:init():867] starting run threads in backend
-2024-11-14 07:55:29,250 INFO    MainThread:770138 [wandb_run.py:_console_start():2456] atexit reg
-2024-11-14 07:55:29,250 INFO    MainThread:770138 [wandb_run.py:_redirect():2305] redirect: wrap_raw
-2024-11-14 07:55:29,250 INFO    MainThread:770138 [wandb_run.py:_redirect():2370] Wrapping output streams.
-2024-11-14 07:55:29,250 INFO    MainThread:770138 [wandb_run.py:_redirect():2395] Redirects installed.
-2024-11-14 07:55:29,251 INFO    MainThread:770138 [wandb_init.py:init():911] run started, returning control to user process
-2024-11-14 07:55:52,466 INFO    MainThread:770138 [wandb_run.py:_config_callback():1387] config_cb None None {'num_classes': 9, 'init_lr': 0.001, 'pretraining': True, 'num_channels': 3, 'use_data_augmentation': False, 'batch_size': 2, 'num_workers': 14, 'nlst_metadata_path': '/scratch/project2/nlst-metadata/full_nlst_google.json', 'valid_exam_path': '/scratch/project2/nlst-metadata/valid_exams.p', 'nlst_dir': '/scratch/project2/compressed', 'lungrads_path': '/scratch/project2/nlst-metadata/nlst_acc2lungrads.p', 'num_images': 200, 'max_followup': 6, 'img_size': [256, 256], 'class_balance': False, 'group_keys': ['race', 'educat', 'gender', 'age', 'ethnic'], 'feature_config': ['age']}
-2024-11-14 07:56:12,054 WARNING MsgRouterThr:770138 [router.py:message_loop():75] message_loop has been closed
diff --git a/wandb/run-20241114_075528-gai09q5t/run-gai09q5t.wandb b/wandb/run-20241114_075528-gai09q5t/run-gai09q5t.wandb
deleted file mode 100644
index e662614..0000000
Binary files a/wandb/run-20241114_075528-gai09q5t/run-gai09q5t.wandb and /dev/null differ
diff --git a/wandb/run-20241114_075801-wjtymsd4/files/code/scripts/main.py b/wandb/run-20241114_075801-wjtymsd4/files/code/scripts/main.py
deleted file mode 100644
index 8ac4d07..0000000
--- a/wandb/run-20241114_075801-wjtymsd4/files/code/scripts/main.py
+++ /dev/null
@@ -1,315 +0,0 @@
-import inspect
-import math
-import argparse
-import sys
-import os
-
-sys.path.append(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))
-from src.lightning import MLP, CNN, LinearModel, ResNet18, RiskModel, ResNet3D, Swin3DModel
-from src.dataset import PathMnist, NLST
-from lightning.pytorch.cli import LightningArgumentParser
-import lightning.pytorch as pl
-from torch.cuda import device_count
-import wandb
-from torch.backends import cudnn
-from lightning import seed_everything
-import json
-
-dirname = os.path.dirname(__file__)
-global_seed = json.load(open(os.path.join(dirname, '..', 'global_seed.json')))['global_seed']
-seed_everything(global_seed)
-cudnn.benchmark = False
-
-NAME_TO_MODEL_CLASS = {
-    "mlp": MLP,
-    "cnn": CNN,
-    # "cnn3d": CNN3D,
-    "linear": LinearModel,
-    "resnet": ResNet18,
-    "resnet3d": ResNet3D,
-    "swin3d": Swin3DModel,
-    "risk_model": RiskModel
-}
-
-
-NAME_TO_DATASET_CLASS = {
-    "pathmnist": PathMnist,
-    "nlst": NLST
-}
-
-MODEL_TO_DATASET = {
-    "mlp": "pathmnist",
-    "cnn": "pathmnist",
-    "cnn3d": "nlst",
-    "linear": "pathmnist",
-    "resnet": "pathmnist",
-    "resnet3d": "nlst",
-    "swin3d": "nlst",
-    "risk_model": "nlst"
-}
-
-dirname = os.path.dirname(__file__)
-
-def add_main_args(parser: LightningArgumentParser) -> LightningArgumentParser:
-
-    parser.add_argument(
-        "--model_name",
-        default="mlp",
-        choices=["mlp", "linear", "cnn", "resnet", "risk_model", "swin3d", "resnet3d"],  
-        help="Name of model to use",
-    )
-
-    parser.add_argument(
-        "--dataset_name",
-        default="pathmnist",
-        choices=["pathmnist", "nlst"],
-        help="Name of dataset to use"
-    )
-
-    parser.add_argument(
-        "--project_name",
-        default="cornerstone_project_2",
-        help="Name of project for wandb"
-    )
-
-    parser.add_argument(
-        "--monitor_key",
-        default="val_loss",
-        help="Name of metric to use for checkpointing. (e.g. val_loss, val_acc)"
-    )
-
-    parser.add_argument(
-        "--checkpoint_path",
-        default=None,
-        help="Path to checkpoint to load from. If None, init from scratch."
-    )
-
-    parser.add_argument(
-        "--train",
-        default=False,
-        type=bool,
-        help="Whether to train the model."
-    )
-
-    parser.add_argument(
-        "--num_layers",
-        default=1,
-        type=int,
-        help="Depth of the model (number of layers)",
-    )
-
-    parser.add_argument(
-        "--use_bn",
-        default=False,
-        type=bool,
-        help="Whether to batch normalize in each layer",
-    )
-
-    parser.add_argument(
-        "--hidden_dim",
-        default=512,
-        type=int,
-        help="The dimension of the hidden layer(s)"
-    )
-
-    parser.add_argument(
-        "--use_data_augmentation",
-        default=False,
-        type=bool,
-        help="Whether to augment the data"
-    )
-
-    parser.add_argument(
-        "--pretraining",
-        default=False,
-        type=bool,
-        help="Whether to use pretrained model weights (only used for resnet)"
-    )
-
-    parser.add_argument(
-        "--wandb_entity",
-        default='CPH29',
-        type=str,
-        help="The wandb account to log metrics and models to"
-    )
-
-    parser.add_argument(
-        "--num_workers",
-        type=int,
-        default=1,
-        help="Number of processes to running in parallel"
-    )
-
-    parser.add_argument(
-        "--class_balance",
-        default=False,
-        type=bool,
-        help="Whether to perform class-balanced sampling (only used for nlst dataset)"
-    )
-
-    parser.add_argument(
-        "--batch_size",
-        default=4,
-        type=int,
-        help="Number of samples per batch"
-    )
-
-    parser.add_argument(
-        "--group_keys",
-        default=['race', 'educat', 'gender', 'age', 'ethnic'],
-        nargs='*',
-        help="The groups to perform subgroup analysis on (only used for nlst dataset)"
-    )
-
-    return parser
-
-def parse_args() -> argparse.Namespace:
-    parser = LightningArgumentParser()
-    parser.add_lightning_class_args(pl.Trainer, nested_key="trainer")
-    for model_name, model_class in NAME_TO_MODEL_CLASS.items():
-        parser.add_lightning_class_args(model_class, nested_key=model_name)
-    for dataset_name, data_class in NAME_TO_DATASET_CLASS.items():
-        parser.add_lightning_class_args(data_class, nested_key=dataset_name)
-    parser = add_main_args(parser)
-    args = parser.parse_args()
-    return args
-
-def get_caller():
-    caller = os.path.split(inspect.getsourcefile(sys._getframe(1)))[-1]
-    return caller
-
-def get_datamodule_num_workers(num_process_workers=None):
-    # set per https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813/5
-    num_process_workers = num_process_workers if num_process_workers else 1
-    datamodule_num_workers = device_count() * 8
-    n_cpus = os.cpu_count()
-    if datamodule_num_workers * num_process_workers >= n_cpus:
-        datamodule_num_workers = math.floor(n_cpus/num_process_workers * .9) 
-    return datamodule_num_workers
-
-def get_datamodule(args):
-    # get workers for datamodule
-    datamodule_num_workers = get_datamodule_num_workers(args.num_workers)
-    
-    # get datamodule args
-    datamodule_vars = vars(vars(args)[args.dataset_name])
-    update_vars = {k:v for k,v in vars(args).items() if k in datamodule_vars}
-    datamodule_vars.update(update_vars)
-    datamodule_vars.update({'num_workers': datamodule_num_workers})
-
-    # init data module
-    datamodule = NAME_TO_DATASET_CLASS[args.dataset_name](**datamodule_vars)
-
-    return datamodule
-
-def get_model(args):
-    print(f"Initializing {args.model_name} model")
-    if args.checkpoint_path is None:
-        model_vars = vars(vars(args)[args.model_name])
-        update_vars = {k:v for k,v in vars(args).items() if k in model_vars}
-        model_vars.update(update_vars)
-        print('with params ', model_vars)
-        model = NAME_TO_MODEL_CLASS[args.model_name](**model_vars)
-    else:
-        model = NAME_TO_MODEL_CLASS[args.model_name].load_from_checkpoint(args.checkpoint_path)
-
-    return model
-
-def get_trainer(args, strategy='ddp', logger=None, callbacks=[]):
-    args.trainer.accelerator = 'auto'
-    args.trainer.strategy = strategy
-    args.trainer.logger = logger
-    args.trainer.precision = "bf16-mixed" ## This mixed precision training is highly recommended
-    args.trainer.min_epochs = 100
-
-    # set checkpoint save directory
-    dirpath = os.path.join(dirname, '../models', args.model_name)
-    if not os.path.isdir(dirpath):
-        os.makedirs(dirpath)
-
-    args.trainer.callbacks = callbacks
-
-    # init trainer
-    trainer_args = vars(args.trainer)
-    trainer = pl.Trainer(**trainer_args)
-
-    return trainer
-
-def get_logger(args):
-    logger = pl.loggers.WandbLogger(project=args.project_name,
-                                    entity=args.wandb_entity,
-                                    group=args.model_name,
-                                    dir=os.path.join(dirname, '..'))
-
-    return logger
-
-def get_callbacks(args):
-    # set checkpoint save directory
-    dirpath = os.path.join(dirname, '../models', args.model_name)
-    if not os.path.isdir(dirpath):
-        os.makedirs(dirpath)
-    
-    callbacks = [
-        pl.callbacks.ModelCheckpoint(
-            monitor=args.monitor_key,
-            mode='min' if "loss" in args.monitor_key else "max",
-            dirpath=dirpath,
-            filename=args.model_name + '-{epoch:002d}-{val_loss:.2f}',
-            save_last=True,
-            every_n_epochs=1
-        ),
-        pl.callbacks.EarlyStopping(
-            monitor=args.monitor_key,
-            mode='min' if "loss" in args.monitor_key else "max",
-            patience=10,
-            check_on_train_epoch_end=True
-        )]
-    
-    return callbacks
-
-
-def main(args: argparse.Namespace):
-    print("Loading data ..")
-    print(f"Training: {args.train}")
-    print(f"Model Name: {args.model_name}")
-    print(f"Dataset Name: {args.dataset_name}")
-    print(f"Pretraining: {args.pretraining}")
-    print(f"Num Workers: {args.num_workers}")
-
-    print("Preparing lighning data module (encapsulates dataset init and data loaders)")
-    """
-        Most the data loading logic is pre-implemented in the LightningDataModule class for you.
-        However, you may want to alter this code for special localization logic or to suit your risk
-        model implementations
-    """
-
-    datamodule = get_datamodule(args)
-    model = get_model(args)
-    logger = get_logger(args)
-    callbacks = get_callbacks(args)
-    trainer = get_trainer(args, callbacks=callbacks, logger=logger)
-
-    if args.train:
-        print("Training model")
-        trainer.fit(model, datamodule)
-
-    print("Best model checkpoint path: ", trainer.checkpoint_callback.best_model_path)
-
-    print("Evaluating model on validation set")
-    trainer.validate(model, datamodule)
-
-    print("Evaluating model on test set")
-    trainer.test(model, datamodule)
-
-    if logger:
-        logger.finalize('success')
-    wandb.finish()
-
-    print("Done")
-
-
-if __name__ == '__main__':
-    __spec__ = None
-    args = parse_args()
-    main(args)
-
diff --git a/wandb/run-20241114_075801-wjtymsd4/files/config.yaml b/wandb/run-20241114_075801-wjtymsd4/files/config.yaml
deleted file mode 100644
index b27a260..0000000
--- a/wandb/run-20241114_075801-wjtymsd4/files/config.yaml
+++ /dev/null
@@ -1,80 +0,0 @@
-_wandb:
-    value:
-        cli_version: 0.18.7
-        code_path: code/scripts/main.py
-        m:
-            - "1": trainer/global_step
-              "6":
-                - 3
-              "7": []
-        python_version: 3.10.15
-        t:
-            "1":
-                - 1
-                - 5
-                - 41
-                - 50
-                - 53
-                - 55
-                - 106
-            "2":
-                - 1
-                - 5
-                - 41
-                - 50
-                - 53
-                - 55
-                - 106
-            "3":
-                - 7
-                - 23
-                - 55
-                - 66
-            "4": 3.10.15
-            "5": 0.18.7
-            "8":
-                - 5
-            "12": 0.18.7
-            "13": linux-x86_64
-batch_size:
-    value: 2
-class_balance:
-    value: false
-feature_config:
-    value:
-        - age
-group_keys:
-    value:
-        - race
-        - educat
-        - gender
-        - age
-        - ethnic
-img_size:
-    value:
-        - 256
-        - 256
-init_lr:
-    value: 0.001
-lungrads_path:
-    value: /scratch/project2/nlst-metadata/nlst_acc2lungrads.p
-max_followup:
-    value: 6
-nlst_dir:
-    value: /scratch/project2/compressed
-nlst_metadata_path:
-    value: /scratch/project2/nlst-metadata/full_nlst_google.json
-num_channels:
-    value: 3
-num_classes:
-    value: 9
-num_images:
-    value: 200
-num_workers:
-    value: 14
-pretraining:
-    value: true
-use_data_augmentation:
-    value: false
-valid_exam_path:
-    value: /scratch/project2/nlst-metadata/valid_exams.p
diff --git a/wandb/run-20241114_075801-wjtymsd4/files/diff_3eca8d6930aef344f1fe58ce268871cfa453ac07.patch b/wandb/run-20241114_075801-wjtymsd4/files/diff_3eca8d6930aef344f1fe58ce268871cfa453ac07.patch
deleted file mode 100644
index fa4d5ff..0000000
--- a/wandb/run-20241114_075801-wjtymsd4/files/diff_3eca8d6930aef344f1fe58ce268871cfa453ac07.patch
+++ /dev/null
@@ -1,20 +0,0 @@
-diff --git a/src/lightning.py b/src/lightning.py
-index 767a5b6..775cb85 100644
---- a/src/lightning.py
-+++ b/src/lightning.py
-@@ -333,6 +333,7 @@ class MLP(Classifer):
-         self.model.apply(self.init_weights)
- 
-     def forward(self, x):
-+        
-         batch_size, channels, width, height = x.size()
-         x = rearrange(x, 'b c w h -> b (w h c)')
-         return self.model(x)
-@@ -454,6 +455,7 @@ class ResNet18(Classifer):
-             self.classifier.apply(self.init_weights)
- 
-     def forward(self, x):
-+        print('Size: ', x.size())
-         batch_size, channels, width, height = x.size()
-         x = rearrange(x, 'b c w h -> b c h w')
-         return self.classifier(x)
diff --git a/wandb/run-20241114_075801-wjtymsd4/files/output.log b/wandb/run-20241114_075801-wjtymsd4/files/output.log
deleted file mode 100644
index 07c09da..0000000
--- a/wandb/run-20241114_075801-wjtymsd4/files/output.log
+++ /dev/null
@@ -1,120 +0,0 @@
-100%|███████████████████████████████████████████████████████████████████████| 15000/15000 [00:00<00:00, 35335.55it/s]
-NLST Dataset. 28161 exams (417.0 with cancer in one year, 1444 cancer ever) from 9646 patients
-NLST Dataset. 6839 exams (99.0 with cancer in one year, 337 cancer ever) from 2336 patients
-NLST Dataset. 6479 exams (86.0 with cancer in one year, 320 cancer ever) from 2204 patients
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
-
-  | Name       | Type               | Params | Mode
-----------------------------------------------------------
-0 | loss       | CrossEntropyLoss   | 0      | train
-1 | accuracy   | MulticlassAccuracy | 0      | train
-2 | auc        | MulticlassAUROC    | 0      | train
-3 | classifier | ResNet             | 11.2 M | train
-----------------------------------------------------------
-11.2 M    Trainable params
-0         Non-trainable params
-11.2 M    Total params
-44.725    Total estimated model params size (MB)
-71        Modules in train mode
-0         Modules in eval mode
-Sanity Checking DataLoader 0:   0%|                                                            | 0/2 [00:00<?, ?it/s]Size:  torch.Size([2, 3, 32, 224, 224])
-Traceback (most recent call last):
-  File "/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py", line 314, in <module>
-    main(args)
-  File "/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py", line 294, in main
-    trainer.fit(model, datamodule)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
-    call._call_and_handle_interrupt(
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
-    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
-    return function(*args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
-    self._run(model, ckpt_path=ckpt_path)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
-    results = self._run_stage()
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1023, in _run_stage
-    self._run_sanity_check()
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1052, in _run_sanity_check
-    val_loop.run()
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py", line 178, in _decorator
-    return loop_run(self, *args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 135, in run
-    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 396, in _evaluation_step
-    output = call._call_strategy_hook(trainer, hook_name, *step_args)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 319, in _call_strategy_hook
-    output = fn(*args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 410, in validation_step
-    return self._forward_redirection(self.model, self.lightning_module, "validation_step", *args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 640, in __call__
-    wrapper_output = wrapper_module(*args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
-    return self._call_impl(*args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
-    return forward_call(*args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
-    else self._run_ddp_forward(*inputs, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
-    return self.module(*inputs, **kwargs)  # type: ignore[index]
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
-    return self._call_impl(*args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
-    return forward_call(*args, **kwargs)
-  File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 633, in wrapped_forward
-    out = method(*_args, **_kwargs)
-  File "/scratch/users/rbhalerao/CPH200A_project2/src/lightning.py", line 71, in validation_step
-    y_hat = self.forward(x)
-  File "/scratch/users/rbhalerao/CPH200A_project2/src/lightning.py", line 459, in forward
-    batch_size, channels, width, height = x.size()
-ValueError: too many values to unpack (expected 4)
-[rank0]: Traceback (most recent call last):
-[rank0]:   File "/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py", line 314, in <module>
-[rank0]:     main(args)
-[rank0]:   File "/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py", line 294, in main
-[rank0]:     trainer.fit(model, datamodule)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
-[rank0]:     call._call_and_handle_interrupt(
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
-[rank0]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
-[rank0]:     return function(*args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
-[rank0]:     self._run(model, ckpt_path=ckpt_path)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
-[rank0]:     results = self._run_stage()
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1023, in _run_stage
-[rank0]:     self._run_sanity_check()
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1052, in _run_sanity_check
-[rank0]:     val_loop.run()
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py", line 178, in _decorator
-[rank0]:     return loop_run(self, *args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 135, in run
-[rank0]:     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 396, in _evaluation_step
-[rank0]:     output = call._call_strategy_hook(trainer, hook_name, *step_args)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 319, in _call_strategy_hook
-[rank0]:     output = fn(*args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 410, in validation_step
-[rank0]:     return self._forward_redirection(self.model, self.lightning_module, "validation_step", *args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 640, in __call__
-[rank0]:     wrapper_output = wrapper_module(*args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
-[rank0]:     return self._call_impl(*args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
-[rank0]:     return forward_call(*args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
-[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
-[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
-[rank0]:     return self._call_impl(*args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
-[rank0]:     return forward_call(*args, **kwargs)
-[rank0]:   File "/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 633, in wrapped_forward
-[rank0]:     out = method(*_args, **_kwargs)
-[rank0]:   File "/scratch/users/rbhalerao/CPH200A_project2/src/lightning.py", line 71, in validation_step
-[rank0]:     y_hat = self.forward(x)
-[rank0]:   File "/scratch/users/rbhalerao/CPH200A_project2/src/lightning.py", line 459, in forward
-[rank0]:     batch_size, channels, width, height = x.size()
-[rank0]: ValueError: too many values to unpack (expected 4)
diff --git a/wandb/run-20241114_075801-wjtymsd4/files/wandb-metadata.json b/wandb/run-20241114_075801-wjtymsd4/files/wandb-metadata.json
deleted file mode 100644
index ef470b7..0000000
--- a/wandb/run-20241114_075801-wjtymsd4/files/wandb-metadata.json
+++ /dev/null
@@ -1,81 +0,0 @@
-{
-  "os": "Linux-6.8.0-1014-nvidia-x86_64-with-glibc2.35",
-  "python": "3.10.15",
-  "startedAt": "2024-11-14T07:58:01.328101Z",
-  "args": [
-    "--model_name",
-    "resnet",
-    "--dataset_name",
-    "nlst",
-    "--train",
-    "True",
-    "--pretraining",
-    "True",
-    "--use_data_augmentation",
-    "False",
-    "--batch_size",
-    "2",
-    "--num_workers",
-    "4",
-    "--project_name",
-    "cornerstone_project_2",
-    "--wandb_entity",
-    "cph29"
-  ],
-  "program": "/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py",
-  "codePath": "scripts/main.py",
-  "git": {
-    "remote": "https://github.com/radhi-bhalerao/CPH200A_project2.git",
-    "commit": "3eca8d6930aef344f1fe58ce268871cfa453ac07"
-  },
-  "email": "rbh@berkeley.edu",
-  "root": ".",
-  "host": "cph-dept-02",
-  "username": "rbhalerao",
-  "executable": "/home/rbhalerao/miniconda3/envs/shree/bin/python",
-  "codePathLocal": "scripts/main.py",
-  "cpu_count": 32,
-  "cpu_count_logical": 64,
-  "gpu": "NVIDIA A40",
-  "gpu_count": 4,
-  "disk": {
-    "/": {
-      "total": "105089261568",
-      "used": "30847905792"
-    }
-  },
-  "memory": {
-    "total": "540682158080"
-  },
-  "cpu": {
-    "count": 32,
-    "countLogical": 64
-  },
-  "gpu_nvidia": [
-    {
-      "name": "NVIDIA A40",
-      "memoryTotal": "48305799168",
-      "cudaCores": 10752,
-      "architecture": "Ampere"
-    },
-    {
-      "name": "NVIDIA A40",
-      "memoryTotal": "48305799168",
-      "cudaCores": 10752,
-      "architecture": "Ampere"
-    },
-    {
-      "name": "NVIDIA A40",
-      "memoryTotal": "48305799168",
-      "cudaCores": 10752,
-      "architecture": "Ampere"
-    },
-    {
-      "name": "NVIDIA A40",
-      "memoryTotal": "48305799168",
-      "cudaCores": 10752,
-      "architecture": "Ampere"
-    }
-  ],
-  "cudaVersion": "12.6"
-}
\ No newline at end of file
diff --git a/wandb/run-20241114_075801-wjtymsd4/files/wandb-summary.json b/wandb/run-20241114_075801-wjtymsd4/files/wandb-summary.json
deleted file mode 100644
index 3dccd4d..0000000
--- a/wandb/run-20241114_075801-wjtymsd4/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"_wandb":{"runtime":44}}
\ No newline at end of file
diff --git a/wandb/run-20241114_075801-wjtymsd4/logs/debug-core.log b/wandb/run-20241114_075801-wjtymsd4/logs/debug-core.log
deleted file mode 120000
index 00cf1c1..0000000
--- a/wandb/run-20241114_075801-wjtymsd4/logs/debug-core.log
+++ /dev/null
@@ -1 +0,0 @@
-/home/rbhalerao/.cache/wandb/logs/core-debug-20241114_075800.log
\ No newline at end of file
diff --git a/wandb/run-20241114_075801-wjtymsd4/logs/debug-internal.log b/wandb/run-20241114_075801-wjtymsd4/logs/debug-internal.log
deleted file mode 100644
index 6d08439..0000000
--- a/wandb/run-20241114_075801-wjtymsd4/logs/debug-internal.log
+++ /dev/null
@@ -1,16 +0,0 @@
-{"time":"2024-11-14T07:58:01.331734992Z","level":"INFO","msg":"using version","core version":"0.18.7"}
-{"time":"2024-11-14T07:58:01.331746373Z","level":"INFO","msg":"created symlink","path":"wandb/run-20241114_075801-wjtymsd4/logs/debug-core.log"}
-{"time":"2024-11-14T07:58:01.4400518Z","level":"INFO","msg":"created new stream","id":"wjtymsd4"}
-{"time":"2024-11-14T07:58:01.440098437Z","level":"INFO","msg":"stream: started","id":"wjtymsd4"}
-{"time":"2024-11-14T07:58:01.44015398Z","level":"INFO","msg":"writer: Do: started","stream_id":"wjtymsd4"}
-{"time":"2024-11-14T07:58:01.440199204Z","level":"INFO","msg":"sender: started","stream_id":"wjtymsd4"}
-{"time":"2024-11-14T07:58:01.440241693Z","level":"INFO","msg":"handler: started","stream_id":"wjtymsd4"}
-{"time":"2024-11-14T07:58:02.257623944Z","level":"INFO","msg":"Starting system monitor"}
-{"time":"2024-11-14T07:58:46.047773807Z","level":"INFO","msg":"stream: closing","id":"wjtymsd4"}
-{"time":"2024-11-14T07:58:46.047809714Z","level":"INFO","msg":"Stopping system monitor"}
-{"time":"2024-11-14T07:58:46.049502668Z","level":"INFO","msg":"Stopped system monitor"}
-{"time":"2024-11-14T07:58:47.760905252Z","level":"INFO","msg":"fileTransfer: Close: file transfer manager closed"}
-{"time":"2024-11-14T07:58:47.902838087Z","level":"INFO","msg":"handler: closed","stream_id":"wjtymsd4"}
-{"time":"2024-11-14T07:58:47.902887189Z","level":"INFO","msg":"writer: Close: closed","stream_id":"wjtymsd4"}
-{"time":"2024-11-14T07:58:47.902915631Z","level":"INFO","msg":"sender: closed","stream_id":"wjtymsd4"}
-{"time":"2024-11-14T07:58:47.902960104Z","level":"INFO","msg":"stream: closed","id":"wjtymsd4"}
diff --git a/wandb/run-20241114_075801-wjtymsd4/logs/debug.log b/wandb/run-20241114_075801-wjtymsd4/logs/debug.log
deleted file mode 100644
index fd0ab4b..0000000
--- a/wandb/run-20241114_075801-wjtymsd4/logs/debug.log
+++ /dev/null
@@ -1,27 +0,0 @@
-2024-11-14 07:58:01,322 INFO    MainThread:797513 [wandb_setup.py:_flush():79] Current SDK version is 0.18.7
-2024-11-14 07:58:01,322 INFO    MainThread:797513 [wandb_setup.py:_flush():79] Configure stats pid to 797513
-2024-11-14 07:58:01,322 INFO    MainThread:797513 [wandb_setup.py:_flush():79] Loading settings from /home/rbhalerao/.config/wandb/settings
-2024-11-14 07:58:01,322 INFO    MainThread:797513 [wandb_setup.py:_flush():79] Loading settings from /scratch/users/rbhalerao/CPH200A_project2/wandb/settings
-2024-11-14 07:58:01,322 INFO    MainThread:797513 [wandb_setup.py:_flush():79] Loading settings from environment variables: {}
-2024-11-14 07:58:01,322 INFO    MainThread:797513 [wandb_setup.py:_flush():79] Applying setup settings: {'mode': None, '_disable_service': None}
-2024-11-14 07:58:01,322 INFO    MainThread:797513 [wandb_setup.py:_flush():79] Inferring run settings from compute environment: {'program_relpath': 'scripts/main.py', 'program_abspath': '/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py', 'program': '/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py'}
-2024-11-14 07:58:01,322 INFO    MainThread:797513 [wandb_setup.py:_flush():79] Applying login settings: {}
-2024-11-14 07:58:01,322 INFO    MainThread:797513 [wandb_init.py:_log_setup():533] Logging user logs to ./wandb/run-20241114_075801-wjtymsd4/logs/debug.log
-2024-11-14 07:58:01,322 INFO    MainThread:797513 [wandb_init.py:_log_setup():534] Logging internal logs to ./wandb/run-20241114_075801-wjtymsd4/logs/debug-internal.log
-2024-11-14 07:58:01,322 INFO    MainThread:797513 [wandb_init.py:init():619] calling init triggers
-2024-11-14 07:58:01,322 INFO    MainThread:797513 [wandb_init.py:init():626] wandb.init called with sweep_config: {}
-config: {}
-2024-11-14 07:58:01,322 INFO    MainThread:797513 [wandb_init.py:init():669] starting backend
-2024-11-14 07:58:01,322 INFO    MainThread:797513 [wandb_init.py:init():673] sending inform_init request
-2024-11-14 07:58:01,327 INFO    MainThread:797513 [backend.py:_multiprocessing_setup():104] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
-2024-11-14 07:58:01,327 INFO    MainThread:797513 [wandb_init.py:init():686] backend started and connected
-2024-11-14 07:58:01,334 INFO    MainThread:797513 [wandb_init.py:init():781] updated telemetry
-2024-11-14 07:58:01,338 INFO    MainThread:797513 [wandb_init.py:init():814] communicating run to backend with 90.0 second timeout
-2024-11-14 07:58:02,254 INFO    MainThread:797513 [wandb_init.py:init():867] starting run threads in backend
-2024-11-14 07:58:02,375 INFO    MainThread:797513 [wandb_run.py:_console_start():2456] atexit reg
-2024-11-14 07:58:02,375 INFO    MainThread:797513 [wandb_run.py:_redirect():2305] redirect: wrap_raw
-2024-11-14 07:58:02,375 INFO    MainThread:797513 [wandb_run.py:_redirect():2370] Wrapping output streams.
-2024-11-14 07:58:02,375 INFO    MainThread:797513 [wandb_run.py:_redirect():2395] Redirects installed.
-2024-11-14 07:58:02,376 INFO    MainThread:797513 [wandb_init.py:init():911] run started, returning control to user process
-2024-11-14 07:58:25,584 INFO    MainThread:797513 [wandb_run.py:_config_callback():1387] config_cb None None {'num_classes': 9, 'init_lr': 0.001, 'pretraining': True, 'num_channels': 3, 'use_data_augmentation': False, 'batch_size': 2, 'num_workers': 14, 'nlst_metadata_path': '/scratch/project2/nlst-metadata/full_nlst_google.json', 'valid_exam_path': '/scratch/project2/nlst-metadata/valid_exams.p', 'nlst_dir': '/scratch/project2/compressed', 'lungrads_path': '/scratch/project2/nlst-metadata/nlst_acc2lungrads.p', 'num_images': 200, 'max_followup': 6, 'img_size': [256, 256], 'class_balance': False, 'group_keys': ['race', 'educat', 'gender', 'age', 'ethnic'], 'feature_config': ['age']}
-2024-11-14 07:58:46,047 WARNING MsgRouterThr:797513 [router.py:message_loop():75] message_loop has been closed
diff --git a/wandb/run-20241114_075801-wjtymsd4/run-wjtymsd4.wandb b/wandb/run-20241114_075801-wjtymsd4/run-wjtymsd4.wandb
deleted file mode 100644
index fb4946d..0000000
Binary files a/wandb/run-20241114_075801-wjtymsd4/run-wjtymsd4.wandb and /dev/null differ
diff --git a/wandb/run-20241114_195653-snr5hkox/files/code/scripts/main.py b/wandb/run-20241114_195653-snr5hkox/files/code/scripts/main.py
deleted file mode 100644
index 5d4588f..0000000
--- a/wandb/run-20241114_195653-snr5hkox/files/code/scripts/main.py
+++ /dev/null
@@ -1,336 +0,0 @@
-import inspect
-import math
-import argparse
-import sys
-import os
-
-sys.path.append(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))
-from src.lightning import MLP, CNN, LinearModel, ResNet18, RiskModel, ResNet3D, Swin3DModel, ResNet18_adapted
-from src.dataset import PathMnist, NLST
-from lightning.pytorch.cli import LightningArgumentParser
-import lightning.pytorch as pl
-from torch.cuda import device_count
-import wandb
-from torch.backends import cudnn
-from lightning import seed_everything
-import json
-
-dirname = os.path.dirname(__file__)
-global_seed = json.load(open(os.path.join(dirname, '..', 'global_seed.json')))['global_seed']
-seed_everything(global_seed)
-cudnn.benchmark = False
-
-NAME_TO_MODEL_CLASS = {
-    "mlp": MLP,
-    "cnn": CNN,
-    # "cnn3d": CNN3D,
-    "linear": LinearModel,
-    "resnet": ResNet18,
-    "resnet_adapt": ResNet18_adapted,
-    "resnet3d": ResNet3D,
-    "swin3d": Swin3DModel,
-    "risk_model": RiskModel
-}
-
-
-NAME_TO_DATASET_CLASS = {
-    "pathmnist": PathMnist,
-    "nlst": NLST
-}
-
-MODEL_TO_DATASET = {
-    "mlp": "pathmnist",
-    "cnn": "pathmnist",
-    "cnn3d": "nlst",
-    "linear": "pathmnist",
-    "resnet": "pathmnist",
-    "resnet_adapt": "nlst",
-    "resnet3d": "nlst",
-    "swin3d": "nlst",
-    "risk_model": "nlst"
-}
-
-dirname = os.path.dirname(__file__)
-
-def add_main_args(parser: LightningArgumentParser) -> LightningArgumentParser:
-
-    parser.add_argument(
-        "--model_name",
-        default="mlp",
-        choices=["mlp", "linear", "cnn", "resnet", "resnet_adapt", "risk_model", "swin3d", "resnet3d"],  
-        help="Name of model to use",
-    )
-
-    parser.add_argument(
-        "--dataset_name",
-        default="pathmnist",
-        choices=["pathmnist", "nlst"],
-        help="Name of dataset to use"
-    )
-
-    parser.add_argument(
-        "--project_name",
-        default="cornerstone_project_2",
-        help="Name of project for wandb"
-    )
-
-    parser.add_argument(
-        "--monitor_key",
-        default="val_loss",
-        help="Name of metric to use for checkpointing. (e.g. val_loss, val_acc)"
-    )
-
-    parser.add_argument(
-        "--checkpoint_path",
-        default=None,
-        help="Path to checkpoint to load from. If None, init from scratch."
-    )
-
-    parser.add_argument(
-        "--train",
-        default=False,
-        type=bool,
-        help="Whether to train the model."
-    )
-
-    parser.add_argument(
-        "--num_layers",
-        default=1,
-        type=int,
-        help="Depth of the model (number of layers)",
-    )
-
-    parser.add_argument(
-        "--use_bn",
-        default=False,
-        type=bool,
-        help="Whether to batch normalize in each layer",
-    )
-
-    parser.add_argument(
-        "--hidden_dim",
-        default=512,
-        type=int,
-        help="The dimension of the hidden layer(s)"
-    )
-
-    parser.add_argument(
-        "--use_data_augmentation",
-        default=False,
-        type=bool,
-        help="Whether to augment the data"
-    )
-
-    parser.add_argument(
-        "--pretraining",
-        default=False,
-        type=bool,
-        help="Whether to use pretrained model weights (only used for resnet)"
-    )
-
-    parser.add_argument(
-        "--wandb_entity",
-        default='CPH29',
-        type=str,
-        help="The wandb account to log metrics and models to"
-    )
-
-    parser.add_argument(
-        "--num_workers",
-        type=int,
-        default=1,
-        help="Number of processes to running in parallel"
-    )
-
-    parser.add_argument(
-        "--class_balance",
-        default=False,
-        type=bool,
-        help="Whether to perform class-balanced sampling (only used for nlst dataset)"
-    )
-
-    parser.add_argument(
-        "--batch_size",
-        default=4,
-        type=int,
-        help="Number of samples per batch"
-    )
-
-    parser.add_argument(
-        "--group_keys",
-        default=['race', 'educat', 'gender', 'age', 'ethnic'],
-        nargs='*',
-        help="The groups to perform subgroup analysis on (only used for nlst dataset)"
-    )
-
-    parser.add_argument(
-    "--depth_handling",
-    default="max_pool",
-    choices=["max_pool", "avg_pool", "slice_attention", "3d_conv"],
-    help="Method to handle depth dimension in ResNet18_adapted"
-)
-
-
-    return parser
-
-def parse_args() -> argparse.Namespace:
-    parser = LightningArgumentParser()
-    parser.add_lightning_class_args(pl.Trainer, nested_key="trainer")
-    for model_name, model_class in NAME_TO_MODEL_CLASS.items():
-        parser.add_lightning_class_args(model_class, nested_key=model_name)
-    for dataset_name, data_class in NAME_TO_DATASET_CLASS.items():
-        parser.add_lightning_class_args(data_class, nested_key=dataset_name)
-    parser = add_main_args(parser)
-    args = parser.parse_args()
-    return args
-
-def get_caller():
-    caller = os.path.split(inspect.getsourcefile(sys._getframe(1)))[-1]
-    return caller
-
-def get_datamodule_num_workers(num_process_workers=None):
-    # set per https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813/5
-    num_process_workers = num_process_workers if num_process_workers else 1
-    datamodule_num_workers = device_count() * 8
-    n_cpus = os.cpu_count()
-    if datamodule_num_workers * num_process_workers >= n_cpus:
-        datamodule_num_workers = math.floor(n_cpus/num_process_workers * .9) 
-    return datamodule_num_workers
-
-def get_datamodule(args):
-    # get workers for datamodule
-    datamodule_num_workers = get_datamodule_num_workers(args.num_workers)
-    
-    # get datamodule args
-    datamodule_vars = vars(vars(args)[args.dataset_name])
-    update_vars = {k:v for k,v in vars(args).items() if k in datamodule_vars}
-    datamodule_vars.update(update_vars)
-    datamodule_vars.update({'num_workers': datamodule_num_workers})
-
-    # init data module
-    datamodule = NAME_TO_DATASET_CLASS[args.dataset_name](**datamodule_vars)
-
-    return datamodule
-
-def get_model(args):
-    print(f"Initializing {args.model_name} model")
-    
-    # Check if the model is `resnet_adapt` to handle the specific `depth_handling` parameter
-    if args.model_name == "resnet_adapt":
-        model_vars = vars(vars(args)[args.model_name])
-        update_vars = {k: v for k, v in vars(args).items() if k in model_vars or k == 'depth_handling'}
-        model_vars.update(update_vars)
-    else:
-        # General model initialization without `depth_handling`
-        model_vars = vars(vars(args)[args.model_name])
-        update_vars = {k: v for k, v in vars(args).items() if k in model_vars}
-        model_vars.update(update_vars)
-    
-    # Initialize the model either from scratch or from a checkpoint
-    if args.checkpoint_path is None:
-        print('with params ', model_vars)
-        model = NAME_TO_MODEL_CLASS[args.model_name](**model_vars)
-    else:
-        model = NAME_TO_MODEL_CLASS[args.model_name].load_from_checkpoint(args.checkpoint_path)
-
-    return model
-
-
-def get_trainer(args, strategy='ddp', logger=None, callbacks=[]):
-    args.trainer.accelerator = 'auto'
-    args.trainer.strategy = strategy
-    args.trainer.logger = logger
-    args.trainer.precision = "bf16-mixed" ## This mixed precision training is highly recommended
-    args.trainer.min_epochs = 100
-
-    # set checkpoint save directory
-    dirpath = os.path.join(dirname, '../models', args.model_name)
-    if not os.path.isdir(dirpath):
-        os.makedirs(dirpath)
-
-    args.trainer.callbacks = callbacks
-
-    # init trainer
-    trainer_args = vars(args.trainer)
-    trainer = pl.Trainer(**trainer_args)
-
-    return trainer
-
-def get_logger(args):
-    logger = pl.loggers.WandbLogger(project=args.project_name,
-                                    entity=args.wandb_entity,
-                                    group=args.model_name,
-                                    dir=os.path.join(dirname, '..'))
-
-    return logger
-
-def get_callbacks(args):
-    # set checkpoint save directory
-    dirpath = os.path.join(dirname, '../models', args.model_name)
-    if not os.path.isdir(dirpath):
-        os.makedirs(dirpath)
-    
-    callbacks = [
-        pl.callbacks.ModelCheckpoint(
-            monitor=args.monitor_key,
-            mode='min' if "loss" in args.monitor_key else "max",
-            dirpath=dirpath,
-            filename=args.model_name + '-{epoch:002d}-{val_loss:.2f}',
-            save_last=True,
-            every_n_epochs=1
-        ),
-        pl.callbacks.EarlyStopping(
-            monitor=args.monitor_key,
-            mode='min' if "loss" in args.monitor_key else "max",
-            patience=10,
-            check_on_train_epoch_end=True
-        )]
-    
-    return callbacks
-
-
-def main(args: argparse.Namespace):
-    print("Loading data ..")
-    print(f"Training: {args.train}")
-    print(f"Model Name: {args.model_name}")
-    print(f"Dataset Name: {args.dataset_name}")
-    print(f"Pretraining: {args.pretraining}")
-    print(f"Num Workers: {args.num_workers}")
-
-    print("Preparing lighning data module (encapsulates dataset init and data loaders)")
-    """
-        Most the data loading logic is pre-implemented in the LightningDataModule class for you.
-        However, you may want to alter this code for special localization logic or to suit your risk
-        model implementations
-    """
-
-    datamodule = get_datamodule(args)
-    model = get_model(args)
-    logger = get_logger(args)
-    callbacks = get_callbacks(args)
-    trainer = get_trainer(args, callbacks=callbacks, logger=logger)
-
-    if args.train:
-        print("Training model")
-        trainer.fit(model, datamodule)
-
-    print("Best model checkpoint path: ", trainer.checkpoint_callback.best_model_path)
-
-    print("Evaluating model on validation set")
-    trainer.validate(model, datamodule)
-
-    print("Evaluating model on test set")
-    trainer.test(model, datamodule)
-
-    if logger:
-        logger.finalize('success')
-    wandb.finish()
-
-    print("Done")
-
-
-if __name__ == '__main__':
-    __spec__ = None
-    args = parse_args()
-    main(args)
-
diff --git a/wandb/run-20241114_195653-snr5hkox/files/config.yaml b/wandb/run-20241114_195653-snr5hkox/files/config.yaml
deleted file mode 100644
index be59517..0000000
--- a/wandb/run-20241114_195653-snr5hkox/files/config.yaml
+++ /dev/null
@@ -1,100 +0,0 @@
-_wandb:
-    value:
-        cli_version: 0.18.7
-        code_path: code/scripts/main.py
-        m:
-            - "1": trainer/global_step
-              "6":
-                - 3
-              "7": []
-            - "1": train_acc
-              "5": 1
-              "6":
-                - 1
-                - 3
-              "7": []
-            - "1": train_loss
-              "5": 1
-              "6":
-                - 1
-                - 3
-              "7": []
-            - "1": epoch
-              "5": 1
-              "6":
-                - 1
-                - 3
-              "7": []
-        python_version: 3.10.15
-        t:
-            "1":
-                - 1
-                - 5
-                - 41
-                - 50
-                - 53
-                - 55
-                - 106
-            "2":
-                - 1
-                - 5
-                - 41
-                - 50
-                - 53
-                - 55
-                - 106
-            "3":
-                - 7
-                - 23
-                - 55
-                - 66
-            "4": 3.10.15
-            "5": 0.18.7
-            "8":
-                - 5
-            "12": 0.18.7
-            "13": linux-x86_64
-batch_size:
-    value: 2
-class_balance:
-    value: false
-depth_handling:
-    value: max_pool
-feature_config:
-    value:
-        - age
-group_keys:
-    value:
-        - race
-        - educat
-        - gender
-        - age
-        - ethnic
-img_size:
-    value:
-        - 256
-        - 256
-init_lr:
-    value: 0.001
-lungrads_path:
-    value: /scratch/project2/nlst-metadata/nlst_acc2lungrads.p
-max_followup:
-    value: 6
-nlst_dir:
-    value: /scratch/project2/compressed
-nlst_metadata_path:
-    value: /scratch/project2/nlst-metadata/full_nlst_google.json
-num_channels:
-    value: 3
-num_classes:
-    value: 9
-num_images:
-    value: 200
-num_workers:
-    value: 14
-pretraining:
-    value: true
-use_data_augmentation:
-    value: false
-valid_exam_path:
-    value: /scratch/project2/nlst-metadata/valid_exams.p
diff --git a/wandb/run-20241114_195653-snr5hkox/files/diff.patch b/wandb/run-20241114_195653-snr5hkox/files/diff.patch
deleted file mode 100644
index b6e4b2b..0000000
--- a/wandb/run-20241114_195653-snr5hkox/files/diff.patch
+++ /dev/null
@@ -1,202 +0,0 @@
-diff --git a/scripts/main.py b/scripts/main.py
-index 8ac4d07..5d4588f 100644
---- a/scripts/main.py
-+++ b/scripts/main.py
-@@ -5,7 +5,7 @@ import sys
- import os
- 
- sys.path.append(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))
--from src.lightning import MLP, CNN, LinearModel, ResNet18, RiskModel, ResNet3D, Swin3DModel
-+from src.lightning import MLP, CNN, LinearModel, ResNet18, RiskModel, ResNet3D, Swin3DModel, ResNet18_adapted
- from src.dataset import PathMnist, NLST
- from lightning.pytorch.cli import LightningArgumentParser
- import lightning.pytorch as pl
-@@ -26,6 +26,7 @@ NAME_TO_MODEL_CLASS = {
-     # "cnn3d": CNN3D,
-     "linear": LinearModel,
-     "resnet": ResNet18,
-+    "resnet_adapt": ResNet18_adapted,
-     "resnet3d": ResNet3D,
-     "swin3d": Swin3DModel,
-     "risk_model": RiskModel
-@@ -43,6 +44,7 @@ MODEL_TO_DATASET = {
-     "cnn3d": "nlst",
-     "linear": "pathmnist",
-     "resnet": "pathmnist",
-+    "resnet_adapt": "nlst",
-     "resnet3d": "nlst",
-     "swin3d": "nlst",
-     "risk_model": "nlst"
-@@ -55,7 +57,7 @@ def add_main_args(parser: LightningArgumentParser) -> LightningArgumentParser:
-     parser.add_argument(
-         "--model_name",
-         default="mlp",
--        choices=["mlp", "linear", "cnn", "resnet", "risk_model", "swin3d", "resnet3d"],  
-+        choices=["mlp", "linear", "cnn", "resnet", "resnet_adapt", "risk_model", "swin3d", "resnet3d"],  
-         help="Name of model to use",
-     )
- 
-@@ -161,6 +163,14 @@ def add_main_args(parser: LightningArgumentParser) -> LightningArgumentParser:
-         help="The groups to perform subgroup analysis on (only used for nlst dataset)"
-     )
- 
-+    parser.add_argument(
-+    "--depth_handling",
-+    default="max_pool",
-+    choices=["max_pool", "avg_pool", "slice_attention", "3d_conv"],
-+    help="Method to handle depth dimension in ResNet18_adapted"
-+)
-+
-+
-     return parser
- 
- def parse_args() -> argparse.Namespace:
-@@ -204,10 +214,20 @@ def get_datamodule(args):
- 
- def get_model(args):
-     print(f"Initializing {args.model_name} model")
--    if args.checkpoint_path is None:
-+    
-+    # Check if the model is `resnet_adapt` to handle the specific `depth_handling` parameter
-+    if args.model_name == "resnet_adapt":
-         model_vars = vars(vars(args)[args.model_name])
--        update_vars = {k:v for k,v in vars(args).items() if k in model_vars}
-+        update_vars = {k: v for k, v in vars(args).items() if k in model_vars or k == 'depth_handling'}
-         model_vars.update(update_vars)
-+    else:
-+        # General model initialization without `depth_handling`
-+        model_vars = vars(vars(args)[args.model_name])
-+        update_vars = {k: v for k, v in vars(args).items() if k in model_vars}
-+        model_vars.update(update_vars)
-+    
-+    # Initialize the model either from scratch or from a checkpoint
-+    if args.checkpoint_path is None:
-         print('with params ', model_vars)
-         model = NAME_TO_MODEL_CLASS[args.model_name](**model_vars)
-     else:
-@@ -215,6 +235,7 @@ def get_model(args):
- 
-     return model
- 
-+
- def get_trainer(args, strategy='ddp', logger=None, callbacks=[]):
-     args.trainer.accelerator = 'auto'
-     args.trainer.strategy = strategy
-diff --git a/src/lightning.py b/src/lightning.py
-index 767a5b6..dd72bdc 100644
---- a/src/lightning.py
-+++ b/src/lightning.py
-@@ -333,6 +333,7 @@ class MLP(Classifer):
-         self.model.apply(self.init_weights)
- 
-     def forward(self, x):
-+        
-         batch_size, channels, width, height = x.size()
-         x = rearrange(x, 'b c w h -> b (w h c)')
-         return self.model(x)
-@@ -454,10 +455,105 @@ class ResNet18(Classifer):
-             self.classifier.apply(self.init_weights)
- 
-     def forward(self, x):
-+        print('Size: ', x.size())
-         batch_size, channels, width, height = x.size()
-         x = rearrange(x, 'b c w h -> b c h w')
-         return self.classifier(x)
- 
-+class ResNet18_adapted(Classifer):
-+    def __init__(self, num_classes=9, init_lr=1e-3, pretraining=False, depth_handling='max_pool', **kwargs):
-+        super().__init__(num_classes=num_classes, init_lr=init_lr)
-+        self.save_hyperparameters()
-+        
-+        # Initialize a ResNet18 model
-+        weights_kwargs = {'weights': models.ResNet18_Weights.DEFAULT} if pretraining else {} 
-+        self.classifier = models.resnet18(**weights_kwargs)
-+        self.classifier.fc = nn.Linear(self.classifier.fc.in_features, num_classes)
-+        
-+        # Add handling for depth dimension
-+        self.depth_handling = depth_handling
-+        
-+        if depth_handling == '3d_conv':
-+            # Replace first conv layer with 3D conv
-+            self.classifier.conv1 = nn.Conv3d(
-+                in_channels=3,
-+                out_channels=64,
-+                kernel_size=(3, 7, 7),
-+                stride=(1, 2, 2),
-+                padding=(1, 3, 3),
-+                bias=False
-+            )
-+            # Add a transition layer after first conv to go back to 2D
-+            self.transition = nn.Sequential(
-+                nn.BatchNorm3d(64),
-+                nn.ReLU(inplace=True),
-+                nn.MaxPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1))
-+            )
-+        
-+        if not pretraining:
-+            self.classifier.apply(self.init_weights)
-+
-+    def forward(self, x):
-+        # x shape: [batch_size, channels, depth, height, width]
-+        batch_size, channels, depth, height, width = x.size()
-+        
-+        if self.depth_handling == 'max_pool':
-+            # Method 1: Max pool across depth dimension
-+            x = x.max(dim=2)[0]  # Shape becomes [batch_size, channels, height, width]
-+            return self.classifier(x)
-+            
-+        elif self.depth_handling == 'avg_pool':
-+            # Method 2: Average pool across depth dimension
-+            x = x.mean(dim=2)  # Shape becomes [batch_size, channels, height, width]
-+            return self.classifier(x)
-+            
-+        elif self.depth_handling == 'slice_attention':
-+            # Method 3: Learn attention weights for each slice
-+            # First, reshape to treat depth as batch dimension
-+            x = x.permute(0, 2, 1, 3, 4).contiguous()
-+            x = x.view(-1, channels, height, width)
-+            
-+            # Get features for each slice
-+            features = self.classifier.conv1(x)
-+            features = self.classifier.bn1(features)
-+            features = self.classifier.relu(features)
-+            features = self.classifier.maxpool(features)
-+            features = self.classifier.layer1(features)
-+            
-+            # Reshape back to include depth
-+            features = features.view(batch_size, depth, -1)
-+            
-+            # Simple attention mechanism
-+            attention_weights = F.softmax(self.classifier.avgpool(features), dim=1)
-+            weighted_features = (features * attention_weights).sum(dim=1)
-+            
-+            # Continue through rest of network
-+            x = self.classifier.layer2(weighted_features)
-+            x = self.classifier.layer3(x)
-+            x = self.classifier.layer4(x)
-+            x = self.classifier.avgpool(x)
-+            x = torch.flatten(x, 1)
-+            return self.classifier.fc(x)
-+            
-+        elif self.depth_handling == '3d_conv':
-+            # Method 4: Start with 3D convolutions
-+            x = self.classifier.conv1(x)
-+            x = self.transition(x)
-+            
-+            # Reshape to 2D after initial 3D conv
-+            x = x.squeeze(2)  # Remove depth dimension
-+            
-+            # Continue through rest of the network
-+            x = self.classifier.bn1(x)
-+            x = self.classifier.relu(x)
-+            x = self.classifier.maxpool(x)
-+            x = self.classifier.layer1(x)
-+            x = self.classifier.layer2(x)
-+            x = self.classifier.layer3(x)
-+            x = self.classifier.layer4(x)
-+            x = self.classifier.avgpool(x)
-+            x = torch.flatten(x, 1)
-+            return self.classifier.fc(x)
- 
- class ResNet3D(Classifer):
-     def __init__(self, num_classes=2, init_lr=1e-3, pretraining=False, **kwargs):
diff --git a/wandb/run-20241114_195653-snr5hkox/files/diff_3eca8d6930aef344f1fe58ce268871cfa453ac07.patch b/wandb/run-20241114_195653-snr5hkox/files/diff_3eca8d6930aef344f1fe58ce268871cfa453ac07.patch
deleted file mode 100644
index b6e4b2b..0000000
--- a/wandb/run-20241114_195653-snr5hkox/files/diff_3eca8d6930aef344f1fe58ce268871cfa453ac07.patch
+++ /dev/null
@@ -1,202 +0,0 @@
-diff --git a/scripts/main.py b/scripts/main.py
-index 8ac4d07..5d4588f 100644
---- a/scripts/main.py
-+++ b/scripts/main.py
-@@ -5,7 +5,7 @@ import sys
- import os
- 
- sys.path.append(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))
--from src.lightning import MLP, CNN, LinearModel, ResNet18, RiskModel, ResNet3D, Swin3DModel
-+from src.lightning import MLP, CNN, LinearModel, ResNet18, RiskModel, ResNet3D, Swin3DModel, ResNet18_adapted
- from src.dataset import PathMnist, NLST
- from lightning.pytorch.cli import LightningArgumentParser
- import lightning.pytorch as pl
-@@ -26,6 +26,7 @@ NAME_TO_MODEL_CLASS = {
-     # "cnn3d": CNN3D,
-     "linear": LinearModel,
-     "resnet": ResNet18,
-+    "resnet_adapt": ResNet18_adapted,
-     "resnet3d": ResNet3D,
-     "swin3d": Swin3DModel,
-     "risk_model": RiskModel
-@@ -43,6 +44,7 @@ MODEL_TO_DATASET = {
-     "cnn3d": "nlst",
-     "linear": "pathmnist",
-     "resnet": "pathmnist",
-+    "resnet_adapt": "nlst",
-     "resnet3d": "nlst",
-     "swin3d": "nlst",
-     "risk_model": "nlst"
-@@ -55,7 +57,7 @@ def add_main_args(parser: LightningArgumentParser) -> LightningArgumentParser:
-     parser.add_argument(
-         "--model_name",
-         default="mlp",
--        choices=["mlp", "linear", "cnn", "resnet", "risk_model", "swin3d", "resnet3d"],  
-+        choices=["mlp", "linear", "cnn", "resnet", "resnet_adapt", "risk_model", "swin3d", "resnet3d"],  
-         help="Name of model to use",
-     )
- 
-@@ -161,6 +163,14 @@ def add_main_args(parser: LightningArgumentParser) -> LightningArgumentParser:
-         help="The groups to perform subgroup analysis on (only used for nlst dataset)"
-     )
- 
-+    parser.add_argument(
-+    "--depth_handling",
-+    default="max_pool",
-+    choices=["max_pool", "avg_pool", "slice_attention", "3d_conv"],
-+    help="Method to handle depth dimension in ResNet18_adapted"
-+)
-+
-+
-     return parser
- 
- def parse_args() -> argparse.Namespace:
-@@ -204,10 +214,20 @@ def get_datamodule(args):
- 
- def get_model(args):
-     print(f"Initializing {args.model_name} model")
--    if args.checkpoint_path is None:
-+    
-+    # Check if the model is `resnet_adapt` to handle the specific `depth_handling` parameter
-+    if args.model_name == "resnet_adapt":
-         model_vars = vars(vars(args)[args.model_name])
--        update_vars = {k:v for k,v in vars(args).items() if k in model_vars}
-+        update_vars = {k: v for k, v in vars(args).items() if k in model_vars or k == 'depth_handling'}
-         model_vars.update(update_vars)
-+    else:
-+        # General model initialization without `depth_handling`
-+        model_vars = vars(vars(args)[args.model_name])
-+        update_vars = {k: v for k, v in vars(args).items() if k in model_vars}
-+        model_vars.update(update_vars)
-+    
-+    # Initialize the model either from scratch or from a checkpoint
-+    if args.checkpoint_path is None:
-         print('with params ', model_vars)
-         model = NAME_TO_MODEL_CLASS[args.model_name](**model_vars)
-     else:
-@@ -215,6 +235,7 @@ def get_model(args):
- 
-     return model
- 
-+
- def get_trainer(args, strategy='ddp', logger=None, callbacks=[]):
-     args.trainer.accelerator = 'auto'
-     args.trainer.strategy = strategy
-diff --git a/src/lightning.py b/src/lightning.py
-index 767a5b6..dd72bdc 100644
---- a/src/lightning.py
-+++ b/src/lightning.py
-@@ -333,6 +333,7 @@ class MLP(Classifer):
-         self.model.apply(self.init_weights)
- 
-     def forward(self, x):
-+        
-         batch_size, channels, width, height = x.size()
-         x = rearrange(x, 'b c w h -> b (w h c)')
-         return self.model(x)
-@@ -454,10 +455,105 @@ class ResNet18(Classifer):
-             self.classifier.apply(self.init_weights)
- 
-     def forward(self, x):
-+        print('Size: ', x.size())
-         batch_size, channels, width, height = x.size()
-         x = rearrange(x, 'b c w h -> b c h w')
-         return self.classifier(x)
- 
-+class ResNet18_adapted(Classifer):
-+    def __init__(self, num_classes=9, init_lr=1e-3, pretraining=False, depth_handling='max_pool', **kwargs):
-+        super().__init__(num_classes=num_classes, init_lr=init_lr)
-+        self.save_hyperparameters()
-+        
-+        # Initialize a ResNet18 model
-+        weights_kwargs = {'weights': models.ResNet18_Weights.DEFAULT} if pretraining else {} 
-+        self.classifier = models.resnet18(**weights_kwargs)
-+        self.classifier.fc = nn.Linear(self.classifier.fc.in_features, num_classes)
-+        
-+        # Add handling for depth dimension
-+        self.depth_handling = depth_handling
-+        
-+        if depth_handling == '3d_conv':
-+            # Replace first conv layer with 3D conv
-+            self.classifier.conv1 = nn.Conv3d(
-+                in_channels=3,
-+                out_channels=64,
-+                kernel_size=(3, 7, 7),
-+                stride=(1, 2, 2),
-+                padding=(1, 3, 3),
-+                bias=False
-+            )
-+            # Add a transition layer after first conv to go back to 2D
-+            self.transition = nn.Sequential(
-+                nn.BatchNorm3d(64),
-+                nn.ReLU(inplace=True),
-+                nn.MaxPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1))
-+            )
-+        
-+        if not pretraining:
-+            self.classifier.apply(self.init_weights)
-+
-+    def forward(self, x):
-+        # x shape: [batch_size, channels, depth, height, width]
-+        batch_size, channels, depth, height, width = x.size()
-+        
-+        if self.depth_handling == 'max_pool':
-+            # Method 1: Max pool across depth dimension
-+            x = x.max(dim=2)[0]  # Shape becomes [batch_size, channels, height, width]
-+            return self.classifier(x)
-+            
-+        elif self.depth_handling == 'avg_pool':
-+            # Method 2: Average pool across depth dimension
-+            x = x.mean(dim=2)  # Shape becomes [batch_size, channels, height, width]
-+            return self.classifier(x)
-+            
-+        elif self.depth_handling == 'slice_attention':
-+            # Method 3: Learn attention weights for each slice
-+            # First, reshape to treat depth as batch dimension
-+            x = x.permute(0, 2, 1, 3, 4).contiguous()
-+            x = x.view(-1, channels, height, width)
-+            
-+            # Get features for each slice
-+            features = self.classifier.conv1(x)
-+            features = self.classifier.bn1(features)
-+            features = self.classifier.relu(features)
-+            features = self.classifier.maxpool(features)
-+            features = self.classifier.layer1(features)
-+            
-+            # Reshape back to include depth
-+            features = features.view(batch_size, depth, -1)
-+            
-+            # Simple attention mechanism
-+            attention_weights = F.softmax(self.classifier.avgpool(features), dim=1)
-+            weighted_features = (features * attention_weights).sum(dim=1)
-+            
-+            # Continue through rest of network
-+            x = self.classifier.layer2(weighted_features)
-+            x = self.classifier.layer3(x)
-+            x = self.classifier.layer4(x)
-+            x = self.classifier.avgpool(x)
-+            x = torch.flatten(x, 1)
-+            return self.classifier.fc(x)
-+            
-+        elif self.depth_handling == '3d_conv':
-+            # Method 4: Start with 3D convolutions
-+            x = self.classifier.conv1(x)
-+            x = self.transition(x)
-+            
-+            # Reshape to 2D after initial 3D conv
-+            x = x.squeeze(2)  # Remove depth dimension
-+            
-+            # Continue through rest of the network
-+            x = self.classifier.bn1(x)
-+            x = self.classifier.relu(x)
-+            x = self.classifier.maxpool(x)
-+            x = self.classifier.layer1(x)
-+            x = self.classifier.layer2(x)
-+            x = self.classifier.layer3(x)
-+            x = self.classifier.layer4(x)
-+            x = self.classifier.avgpool(x)
-+            x = torch.flatten(x, 1)
-+            return self.classifier.fc(x)
- 
- class ResNet3D(Classifer):
-     def __init__(self, num_classes=2, init_lr=1e-3, pretraining=False, **kwargs):
diff --git a/wandb/run-20241114_195653-snr5hkox/files/output.log b/wandb/run-20241114_195653-snr5hkox/files/output.log
deleted file mode 100644
index 7ced7c4..0000000
--- a/wandb/run-20241114_195653-snr5hkox/files/output.log
+++ /dev/null
@@ -1,26 +0,0 @@
-100%|██████████████████████████████████████████████████████| 15000/15000 [00:00<00:00, 35948.51it/s]
-NLST Dataset. 28161 exams (417.0 with cancer in one year, 1444 cancer ever) from 9646 patients
-NLST Dataset. 6839 exams (99.0 with cancer in one year, 337 cancer ever) from 2336 patients
-NLST Dataset. 6479 exams (86.0 with cancer in one year, 320 cancer ever) from 2204 patients
-LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
-
-  | Name       | Type               | Params | Mode
-----------------------------------------------------------
-0 | loss       | CrossEntropyLoss   | 0      | train
-1 | accuracy   | MulticlassAccuracy | 0      | train
-2 | auc        | MulticlassAUROC    | 0      | train
-3 | classifier | ResNet             | 11.2 M | train
-----------------------------------------------------------
-11.2 M    Trainable params
-0         Non-trainable params
-11.2 M    Total params
-44.725    Total estimated model params size (MB)
-71        Modules in train mode
-0         Modules in eval mode
-Epoch 0:   3%|    | 99/3521 [01:23<48:00,  1.19it/s, v_num=hkox, train_acc=1.000, train_loss=0.0219]
-/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score
-  warnings.warn(*args, **kwargs)  # noqa: B028
-/home/rbhalerao/miniconda3/envs/shree/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
-  warnings.warn(*args, **kwargs)  # noqa: B028
-
-Detected KeyboardInterrupt, attempting graceful shutdown ...
diff --git a/wandb/run-20241114_195653-snr5hkox/files/requirements.txt b/wandb/run-20241114_195653-snr5hkox/files/requirements.txt
deleted file mode 100644
index 58a7c9c..0000000
--- a/wandb/run-20241114_195653-snr5hkox/files/requirements.txt
+++ /dev/null
@@ -1,137 +0,0 @@
-numpy==2.1.3
-scikit-learn==1.5.2
-nvidia-cublas-cu12==12.4.5.8
-attrs==24.2.0
-sentry-sdk==2.18.0
-nvidia-cuda-cupti-cu12==12.4.127
-aiohappyeyeballs==2.4.3
-networkx==3.4.2
-certifi==2024.8.30
-nibabel==5.3.2
-pyparsing==3.2.0
-humanize==4.11.0
-torchio==0.20.1
-aiosignal==1.3.1
-protobuf==5.28.3
-autograd==1.7.0
-imageio==2.36.0
-torch==2.5.1
-matplotlib==3.9.2
-fonttools==4.54.1
-multidict==6.1.0
-click==8.1.7
-importlib_resources==6.4.5
-docstring_parser==0.16
-tifffile==2024.9.20
-nvidia-cuda-nvrtc-cu12==12.4.127
-medmnist==3.0.2
-nvidia-cusparse-cu12==12.3.1.170
-markdown-it-py==3.0.0
-mdurl==0.1.2
-hydra-core==1.3.2
-kiwisolver==1.4.7
-mpmath==1.3.0
-nvidia-cusolver-cu12==11.6.1.9
-nvidia-curand-cu12==10.3.5.147
-opencv-python-headless==4.10.0.84
-fsspec==2024.10.0
-wrapt==1.16.0
-nvidia-cufft-cu12==11.2.1.3
-autograd-gamma==0.5.0
-GitPython==3.1.43
-scipy==1.14.1
-pydantic==2.9.2
-Deprecated==1.2.14
-Pygments==2.18.0
-wandb==0.18.7
-simsimd==6.0.5
-python-dateutil==2.9.0.post0
-lifelines==0.30.0
-torchvision==0.20.1
-psutil==6.1.0
-tensorboardX==2.6.2.2
-termcolor==2.5.0
-nvidia-cudnn-cu12==9.1.0.70
-pydicom==3.0.1
-frozenlist==1.5.0
-SimpleITK==2.4.0
-idna==3.10
-joblib==1.4.2
-jsonargparse==4.34.0
-threadpoolctl==3.5.0
-sympy==1.13.1
-yarl==1.17.1
-pytorch-lightning==2.4.0
-pillow==11.0.0
-nvidia-nvtx-cu12==12.4.127
-albumentations==1.4.21
-triton==3.1.0
-typing_extensions==4.12.2
-tqdm==4.67.0
-nvidia-cuda-runtime-cu12==12.4.127
-nvidia-nccl-cu12==2.21.5
-pydantic_core==2.23.4
-urllib3==2.2.3
-lazy_loader==0.4
-pytz==2024.2
-six==1.16.0
-lightning-utilities==0.11.8
-eval_type_backport==0.2.0
-gitdb==4.0.11
-MarkupSafe==3.0.2
-docker-pycreds==0.4.0
-interface-meta==1.3.0
-PyYAML==6.0.2
-fire==0.7.0
-contourpy==1.3.1
-formulaic==1.0.2
-platformdirs==4.3.6
-cycler==0.12.1
-omegaconf==2.3.0
-tzdata==2024.2
-rich==13.9.4
-requests==2.32.3
-pip==24.2
-pandas==2.2.3
-setproctitle==1.3.3
-bitsandbytes==0.44.1
-scikit-image==0.24.0
-typeshed_client==2.7.0
-albucore==0.0.20
-typer==0.13.0
-filelock==3.16.1
-antlr4-python3-runtime==4.9.3
-smmap==5.0.1
-packaging==24.2
-async-timeout==5.0.1
-aiohttp==3.11.0
-stringzilla==3.10.10
-wheel==0.44.0
-charset-normalizer==3.4.0
-Jinja2==3.1.4
-setuptools==75.1.0
-einops==0.8.0
-lightning==2.4.0
-shellingham==1.5.4
-torchmetrics==1.6.0
-nvidia-nvjitlink-cu12==12.4.127
-annotated-types==0.7.0
-propcache==0.2.0
-opencv-python==4.10.0.84
-autocommand==2.2.2
-zipp==3.19.2
-wheel==0.43.0
-more-itertools==10.3.0
-jaraco.context==5.3.0
-backports.tarfile==1.2.0
-typeguard==4.3.0
-jaraco.functools==4.0.1
-tomli==2.0.1
-jaraco.text==3.12.1
-typing_extensions==4.12.2
-importlib_resources==6.4.0
-inflect==7.3.1
-importlib_metadata==8.0.0
-packaging==24.1
-jaraco.collections==5.1.0
-platformdirs==4.2.2
diff --git a/wandb/run-20241114_195653-snr5hkox/files/wandb-metadata.json b/wandb/run-20241114_195653-snr5hkox/files/wandb-metadata.json
deleted file mode 100644
index 94c1fe9..0000000
--- a/wandb/run-20241114_195653-snr5hkox/files/wandb-metadata.json
+++ /dev/null
@@ -1,83 +0,0 @@
-{
-  "os": "Linux-6.8.0-1014-nvidia-x86_64-with-glibc2.35",
-  "python": "3.10.15",
-  "startedAt": "2024-11-14T19:56:53.828230Z",
-  "args": [
-    "--model_name",
-    "resnet_adapt",
-    "--dataset_name",
-    "nlst",
-    "--train",
-    "True",
-    "--pretraining",
-    "True",
-    "--use_data_augmentation",
-    "False",
-    "--batch_size",
-    "2",
-    "--num_workers",
-    "4",
-    "--project_name",
-    "cornerstone_project_2",
-    "--wandb_entity",
-    "cph29",
-    "--depth_handling",
-    "max_pool"
-  ],
-  "program": "/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py",
-  "codePath": "scripts/main.py",
-  "git": {
-    "remote": "https://github.com/radhi-bhalerao/CPH200A_project2.git",
-    "commit": "3eca8d6930aef344f1fe58ce268871cfa453ac07"
-  },
-  "email": "rbh@berkeley.edu",
-  "root": ".",
-  "host": "cph-dept-02",
-  "username": "rbhalerao",
-  "executable": "/home/rbhalerao/miniconda3/envs/shree/bin/python",
-  "codePathLocal": "scripts/main.py",
-  "cpu_count": 32,
-  "cpu_count_logical": 64,
-  "gpu": "NVIDIA A40",
-  "gpu_count": 4,
-  "disk": {
-    "/": {
-      "total": "105089261568",
-      "used": "30848516096"
-    }
-  },
-  "memory": {
-    "total": "540682158080"
-  },
-  "cpu": {
-    "count": 32,
-    "countLogical": 64
-  },
-  "gpu_nvidia": [
-    {
-      "name": "NVIDIA A40",
-      "memoryTotal": "48305799168",
-      "cudaCores": 10752,
-      "architecture": "Ampere"
-    },
-    {
-      "name": "NVIDIA A40",
-      "memoryTotal": "48305799168",
-      "cudaCores": 10752,
-      "architecture": "Ampere"
-    },
-    {
-      "name": "NVIDIA A40",
-      "memoryTotal": "48305799168",
-      "cudaCores": 10752,
-      "architecture": "Ampere"
-    },
-    {
-      "name": "NVIDIA A40",
-      "memoryTotal": "48305799168",
-      "cudaCores": 10752,
-      "architecture": "Ampere"
-    }
-  ],
-  "cudaVersion": "12.6"
-}
\ No newline at end of file
diff --git a/wandb/run-20241114_195653-snr5hkox/files/wandb-summary.json b/wandb/run-20241114_195653-snr5hkox/files/wandb-summary.json
deleted file mode 100644
index b568ec2..0000000
--- a/wandb/run-20241114_195653-snr5hkox/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"_step":0,"_wandb":{"runtime":137},"train_acc":1,"train_loss":0.0145263671875,"epoch":0,"trainer/global_step":49,"_timestamp":1.731614313480153e+09,"_runtime":99.653713745}
\ No newline at end of file
diff --git a/wandb/run-20241114_195653-snr5hkox/logs/debug-core.log b/wandb/run-20241114_195653-snr5hkox/logs/debug-core.log
deleted file mode 120000
index 5e9783e..0000000
--- a/wandb/run-20241114_195653-snr5hkox/logs/debug-core.log
+++ /dev/null
@@ -1 +0,0 @@
-/home/rbhalerao/.cache/wandb/logs/core-debug-20241114_195653.log
\ No newline at end of file
diff --git a/wandb/run-20241114_195653-snr5hkox/logs/debug-internal.log b/wandb/run-20241114_195653-snr5hkox/logs/debug-internal.log
deleted file mode 100644
index bf5f6ca..0000000
--- a/wandb/run-20241114_195653-snr5hkox/logs/debug-internal.log
+++ /dev/null
@@ -1,11 +0,0 @@
-{"time":"2024-11-14T19:56:53.831041922Z","level":"INFO","msg":"using version","core version":"0.18.7"}
-{"time":"2024-11-14T19:56:53.831054255Z","level":"INFO","msg":"created symlink","path":"wandb/run-20241114_195653-snr5hkox/logs/debug-core.log"}
-{"time":"2024-11-14T19:56:53.936846337Z","level":"INFO","msg":"created new stream","id":"snr5hkox"}
-{"time":"2024-11-14T19:56:53.936864551Z","level":"INFO","msg":"stream: started","id":"snr5hkox"}
-{"time":"2024-11-14T19:56:53.936943291Z","level":"INFO","msg":"writer: Do: started","stream_id":"snr5hkox"}
-{"time":"2024-11-14T19:56:53.936984028Z","level":"INFO","msg":"sender: started","stream_id":"snr5hkox"}
-{"time":"2024-11-14T19:56:53.937072947Z","level":"INFO","msg":"handler: started","stream_id":"snr5hkox"}
-{"time":"2024-11-14T19:56:54.216084943Z","level":"INFO","msg":"Starting system monitor"}
-{"time":"2024-11-14T19:59:10.912420908Z","level":"INFO","msg":"stream: closing","id":"snr5hkox"}
-{"time":"2024-11-14T19:59:10.912504737Z","level":"INFO","msg":"Stopping system monitor"}
-{"time":"2024-11-14T19:59:10.91411414Z","level":"INFO","msg":"Stopped system monitor"}
diff --git a/wandb/run-20241114_195653-snr5hkox/logs/debug.log b/wandb/run-20241114_195653-snr5hkox/logs/debug.log
deleted file mode 100644
index 722f76c..0000000
--- a/wandb/run-20241114_195653-snr5hkox/logs/debug.log
+++ /dev/null
@@ -1,27 +0,0 @@
-2024-11-14 19:56:53,824 INFO    MainThread:960424 [wandb_setup.py:_flush():79] Current SDK version is 0.18.7
-2024-11-14 19:56:53,824 INFO    MainThread:960424 [wandb_setup.py:_flush():79] Configure stats pid to 960424
-2024-11-14 19:56:53,824 INFO    MainThread:960424 [wandb_setup.py:_flush():79] Loading settings from /home/rbhalerao/.config/wandb/settings
-2024-11-14 19:56:53,824 INFO    MainThread:960424 [wandb_setup.py:_flush():79] Loading settings from /scratch/users/rbhalerao/CPH200A_project2/wandb/settings
-2024-11-14 19:56:53,824 INFO    MainThread:960424 [wandb_setup.py:_flush():79] Loading settings from environment variables: {}
-2024-11-14 19:56:53,824 INFO    MainThread:960424 [wandb_setup.py:_flush():79] Applying setup settings: {'mode': None, '_disable_service': None}
-2024-11-14 19:56:53,824 INFO    MainThread:960424 [wandb_setup.py:_flush():79] Inferring run settings from compute environment: {'program_relpath': 'scripts/main.py', 'program_abspath': '/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py', 'program': '/scratch/users/rbhalerao/CPH200A_project2/scripts/main.py'}
-2024-11-14 19:56:53,824 INFO    MainThread:960424 [wandb_setup.py:_flush():79] Applying login settings: {}
-2024-11-14 19:56:53,824 INFO    MainThread:960424 [wandb_init.py:_log_setup():533] Logging user logs to ./wandb/run-20241114_195653-snr5hkox/logs/debug.log
-2024-11-14 19:56:53,824 INFO    MainThread:960424 [wandb_init.py:_log_setup():534] Logging internal logs to ./wandb/run-20241114_195653-snr5hkox/logs/debug-internal.log
-2024-11-14 19:56:53,825 INFO    MainThread:960424 [wandb_init.py:init():619] calling init triggers
-2024-11-14 19:56:53,825 INFO    MainThread:960424 [wandb_init.py:init():626] wandb.init called with sweep_config: {}
-config: {}
-2024-11-14 19:56:53,825 INFO    MainThread:960424 [wandb_init.py:init():669] starting backend
-2024-11-14 19:56:53,825 INFO    MainThread:960424 [wandb_init.py:init():673] sending inform_init request
-2024-11-14 19:56:53,827 INFO    MainThread:960424 [backend.py:_multiprocessing_setup():104] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
-2024-11-14 19:56:53,828 INFO    MainThread:960424 [wandb_init.py:init():686] backend started and connected
-2024-11-14 19:56:53,833 INFO    MainThread:960424 [wandb_init.py:init():781] updated telemetry
-2024-11-14 19:56:53,838 INFO    MainThread:960424 [wandb_init.py:init():814] communicating run to backend with 90.0 second timeout
-2024-11-14 19:56:54,212 INFO    MainThread:960424 [wandb_init.py:init():867] starting run threads in backend
-2024-11-14 19:56:54,331 INFO    MainThread:960424 [wandb_run.py:_console_start():2456] atexit reg
-2024-11-14 19:56:54,331 INFO    MainThread:960424 [wandb_run.py:_redirect():2305] redirect: wrap_raw
-2024-11-14 19:56:54,331 INFO    MainThread:960424 [wandb_run.py:_redirect():2370] Wrapping output streams.
-2024-11-14 19:56:54,331 INFO    MainThread:960424 [wandb_run.py:_redirect():2395] Redirects installed.
-2024-11-14 19:56:54,332 INFO    MainThread:960424 [wandb_init.py:init():911] run started, returning control to user process
-2024-11-14 19:57:17,662 INFO    MainThread:960424 [wandb_run.py:_config_callback():1387] config_cb None None {'num_classes': 9, 'init_lr': 0.001, 'pretraining': True, 'depth_handling': 'max_pool', 'num_channels': 3, 'use_data_augmentation': False, 'batch_size': 2, 'num_workers': 14, 'nlst_metadata_path': '/scratch/project2/nlst-metadata/full_nlst_google.json', 'valid_exam_path': '/scratch/project2/nlst-metadata/valid_exams.p', 'nlst_dir': '/scratch/project2/compressed', 'lungrads_path': '/scratch/project2/nlst-metadata/nlst_acc2lungrads.p', 'num_images': 200, 'max_followup': 6, 'img_size': [256, 256], 'class_balance': False, 'group_keys': ['race', 'educat', 'gender', 'age', 'ethnic'], 'feature_config': ['age']}
-2024-11-14 19:59:10,911 WARNING MsgRouterThr:960424 [router.py:message_loop():75] message_loop has been closed
diff --git a/wandb/run-20241114_195653-snr5hkox/run-snr5hkox.wandb b/wandb/run-20241114_195653-snr5hkox/run-snr5hkox.wandb
deleted file mode 100644
index 20e160f..0000000
Binary files a/wandb/run-20241114_195653-snr5hkox/run-snr5hkox.wandb and /dev/null differ
